## 1 概述

### 1.1 数据预处理与特征工程

#### 1.1.1数据预处理

##### 定义

数据预处理是从数据中检测，纠正或删除损坏，不准确或不适用于模型的记录的过程。可能面对的问题有：**数据类型不同**，比如有的是文字，有的是数字，有的含时间序列，有的连续，有的间断。也可能，**数据的质量不行**，有噪声，有异常，有缺失，数据出错，量纲不一，有重复，数据是偏态，数据量太大或太小

##### 目的

让数据适应模型，匹配模型的需求

#### 1.1.2 特征工程

##### 定义

特征工程是将原始数据转换为**更能代表预测模型**的潜在问题的特征的过程，可以通过挑选最相关的特征，提取
特征以及创造特征来实现。其中创造特征又经常以降维算法的方式实现。可能面对的问题有：**特征之间有相关性**，**特征和标签无关**，**特征太多或太小**，或者干脆就无法表现出应有的数据现象或无法展示数据的真实面貌

##### 目的

特征工程的目的：降低计算成本，提升模型

### 1.2 sklearn中的数据预处理和特征工程

模块preprocessing：几乎包含数据预处理的所有内容
模块Impute：填补缺失值专用
模块feature_selection：包含特征选择的各种方法的实践
模块decomposition：包含降维算法

<br>

## 2 数据预处理 Preprocessing & Impute

### 2.1 数据无量纲化

数据的无量纲化可以是线性的，也可以是非线性的。

线性的无量纲化包括**中心化**（Zero-centered或者Meansubtraction）处理和**缩放**处理（Scale）。中心化的本质是让所有记录减去一个固定值，即让数据样本数据平移到某个位置。缩放的本质是通过除以一个固定值，将数据固定在某个范围之中，取对数也算是一种缩放处理

#### 2.1.1 preprocessing.MinMaxScaler

当数据(x)按照**最小值中心化**后，再按**极差（最大值 - 最小值）缩放**，数据移动了最小值个单位，并且会被收敛到
[0,1]之间，而这个过程，就叫做数据归一化(Normalization，又称Min-Max Scaling)。注意，**Normalization是归一化，不是正则化**，真正的正则化是regularization，不是数据预处理的一种手段。归一化之后的数据服从正态分布，公式如下：
$$
x^* = \frac{x - min(x)}{max(x) - min(x)}
$$

##### 注:公式分子为最小值中心化，分母为极差缩放

##### 1. 使用sklearn归一化

```python
import pandas as pd
from sklearn.preprocessing import MinMaxScaler

data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]
pd.DataFrame(data)

#实现归一化
scaler = MinMaxScaler() #实例化
scaler = scaler.fit(data) #fit，在这里本质是生成min(x)和max(x)
result = scaler.transform(data) #通过接口导出结果
result

#将归一化后的结果逆转
scaler.inverse_transform(result)

#使用MinMaxScaler的参数feature_range实现将数据归一化到[0,1]以外的范围中
data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]
scaler = MinMaxScaler(feature_range=[5,10]) #依然实例化

#训练和导出结果一步达成
result_ = scaler.fit_transform(data)
result_
```

##### 2. 使用numpy归一化

```python
import numpy as np

X = np.array([[-1, 2], [-0.5, 6], [0, 10], [1, 18]])

#归一化
X_nor = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))
X_nor

#逆转归一化
X_returned = X_nor * (X.max(axis=0) - X.min(axis=0)) + X.min(axis=0)
X_returned

```

#### 2.1.2 StandardScaler和MinMaxScaler选哪个？

**StandardScaler:**

大多数机器学习算法中，会选择StandardScaler来进行特征缩放，因为**MinMaxScaler对异常值非常敏**
**感**。在PCA，聚类，逻辑回归，支持向量机，神经网络这些算法中，StandardScaler往往是最好的选择。
**MinMaxScaler:**

在不涉及距离度量、梯度、协方差计算以及数据需要被压缩到特定区间时使用广泛，比如**数字图像**
**处理中量化像素强度**时，都会使用MinMaxScaler将数据压缩于[0,1]区间之中。

![001- 数据无量纲化选择](D:\Machine_Learning\sklearn\3-数据预处理和特征工程\images\001- 数据无量纲化选择.png)

### 2.2 缺失值处理

##### 1. impute.SimpleImputer填补缺失值

```python
import numpy as np
import pandas as pd
from sklearn.impute import SimpleImputer

data = pd.read_csv('./datas/Narrativedata.csv',index_col=0)
data.info()

#填补年龄
Age = data.loc[:,"Age"].values.reshape(-1,1) #sklearn当中特征矩阵必须是二维

#实例化与不同方法填补
imp_mean = SimpleImputer() #默认均值填补
imp_median = SimpleImputer(strategy="median") #用中位数填补
imp_0 = SimpleImputer(strategy="constant",fill_value=0) #用0填补

#完成调取结果
imp_mean = imp_mean.fit_transform(Age)
imp_median = imp_median.fit_transform(Age)
imp_0 = imp_0.fit_transform(Age)

#在这里我们使用中位数填补Age
data.loc[:,"Age"] = imp_median

#使用众数填补Embarked
Embarked = data.loc[:,"Embarked"].values.reshape(-1,1)
imp_mode = SimpleImputer(strategy = "most_frequent")
data.loc[:,"Embarked"] = imp_mode.fit_transform(Embarked)

#查看更改后的数据
data.info()
```

![002-随机森林填补缺失值参数](D:\Machine_Learning\sklearn\3-数据预处理和特征工程\images\002-随机森林填补缺失值参数.png)

##### 2. Pandas和Numpy填补缺失值

```python
import numpy as np
import pandas as pd
from sklearn.impute import SimpleImputer

data = pd.read_csv('./datas/Narrativedata.csv',index_col=0)
data.info()

#中位数填补年龄
data.loc[:,"Age"] = data.loc[:,"Age"].fillna(data.loc[:,"Age"].median())

#删除nan值
data.dropna(axis=0,inplace=True)

#查看数据
data.info()
```

### 2.3 处理分类型特征：编码与哑变量

在现实中，许多标签和特征在数据收集完毕的时候，都不是以数字来表现的。比如说，学历的取值可以是["小学"，“初中”，“高中”，"大学"]，付费方式可能包含["支付宝"，“现金”，“微信”]等等。在这种情况下，为了让数据适
应算法和库，我们必须将数据进行编码，即是说，将文字型数据转换为数值型。

##### 1. preprocessing.LabelEncoder:标签专用

```python
import numpy as np
import pandas as pd
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OrdinalEncoder
from sklearn.preprocessing import OneHotEncoder

data = pd.read_csv('./datas/Narrativedata.csv',index_col=0)
data.head()

# 缺失值处理

#填补年龄
Age = data.loc[:,"Age"].values.reshape(-1,1) #sklearn当中特征矩阵必须是二维

#实例化与不同方法填补
imp_mean = SimpleImputer() #默认均值填补
imp_median = SimpleImputer(strategy="median") #用中位数填补
imp_0 = SimpleImputer(strategy="constant",fill_value=0) #用0填补

#完成调取结果
imp_mean = imp_mean.fit_transform(Age)
imp_median = imp_median.fit_transform(Age)
imp_0 = imp_0.fit_transform(Age)

#在这里我们使用中位数填补Age
data.loc[:,"Age"] = imp_median

#使用众数填补Embarked
Embarked = data.loc[:,"Embarked"].values.reshape(-1,1)
imp_mode = SimpleImputer(strategy = "most_frequent")
data.loc[:,"Embarked"] = imp_mode.fit_transform(Embarked)

#查看更改后的数据
data.info()

# preprocessing.LabelEncoder:标签专用

y = data.iloc[:,-1] #要输入的是标签，不是特征矩阵，所以允许一维
le = LabelEncoder() #实例化
label = le.fit_transform(y) #调取结果
print(le.classes_) #查看标签中究竟有多少类别
print()

y1 = le.inverse_transform(label) #逆转
print(y1)

data.iloc[:,-1] = label #让标签等于我们运行出来的结果
data.head()

#一般写法:data.iloc[:,-1] = LabelEncoder().fit_transform(data.iloc[:,-1])
```

##### 2. preprocessing.OrdinalEncoder：特征专用

```python
# preprocessing.OrdinalEncoder：特征专用

data_ = data.copy()
data_.head()
OrdinalEncoder().fit(data_.iloc[:,1:-1]).categories_ #对应LabelEncoder的接口classes_，一模一样的功能
data_.iloc[:,1:-1] = OrdinalEncoder().fit_transform(data_.iloc[:,1:-1])
data_.head()
```

##### 3. preprocessing.OneHotEncoder：独热编码，创建哑变量

我们刚才已经用OrdinalEncoder把分类变量Sex和Embarked都转换成数字对应的类别了。在舱门Embarked这一
列中，我们使用[0,1,2]代表了三个不同的舱门，然而这种转换是正确的吗？
我们来思考三种不同性质的分类数据：
1） 舱门（S，C，Q）
三种取值S，C，Q是相互独立的，彼此之间完全没有联系，表达的是S≠C≠Q的概念。这是**名义变量**。
2） 学历（小学，初中，高中）
三种取值不是完全独立的，我们可以明显看出，在性质上可以有高中>初中>小学这样的联系，学历有高低，但是学历取值之间却不是可以计算的，我们不能说小学 + 某个取值 = 初中。这是**有序变量**。
3） 体重（>45kg，>90kg，>135kg）
各个取值之间有联系，且是可以互相计算的，比如120kg - 45kg = 90kg，分类之间可以通过数学计算互相转换。这是**有距变量**。
然而在对特征进行编码的时候，这三种分类数据都会被我们转换为[0,1,2]，这三个数字在算法看来，是连续且可以
计算的，这三个数字相互不等，有大小，并且有着可以相加相乘的联系。所**以算法会把舱门，学历这样的分类特**
**征，都误会成是体重这样的分类特征**。这是说，我们把分类转换成数字的时候，**忽略了数字中自带的数学性质**，所
以给算法传达了一些不准确的信息，而这会影响我们的建模。
类别OrdinalEncoder可以用来处理有序变量，但对于名义变量，我们只有使用哑变量的方式来处理，才能够尽量
向算法传达最准确的信息：

```python
# preprocessing.OneHotEncoder：独热编码，创建哑变量

X = data.iloc[:,1:-1]

enc = OneHotEncoder(categories='auto').fit(X)
result = enc.transform(X).toarray()
print(result)
print()

#一步到位写法
#OneHotEncoder(categories='auto').fit_transform(X).toarray()

#依然可以还原
pd.DataFrame(enc.inverse_transform(result))
print(enc.get_feature_names()) #返回每一个稀疏矩阵的列名

#axis=1,表示跨行进行合并，也就是将量表左右相连，如果是axis=0，就是将量表上下相连
newdata = pd.concat([data,pd.DataFrame(result)],axis=1)
newdata.drop(["Sex","Embarked"],axis=1,inplace=True)
newdata.columns = ["Age","Survived","Female","Male","Embarked_C","Embarked_Q","Embarked_S"]
newdata.head()
```

![003-编码与哑变量](D:\Machine_Learning\sklearn\3-数据预处理和特征工程\images\003-编码与哑变量.png)

### 2.4 处理连续型特征：二值化与分段

##### 1. preprocessing.Binarizer

根据阈值将数据二值化（将特征值设置为0或1），用于处理连续型变量。大于阈值的值映射为1，而小于或等于阈
值的值映射为0。默认阈值为0时，特征中所有的正值都映射到1。二值化是对文本计数数据的常见操作，分析人员
可以决定仅考虑某种现象的存在与否。它还可以用作考虑布尔随机变量的估计器的预处理步骤（例如，使用贝叶斯
设置中的伯努利分布建模）。

```python
# preprocessing.Binarizer

#将年龄二值化
data_2 = data.copy()
X = data_2.iloc[:,0].values.reshape(-1,1) #类为特征专用，所以不能使用一维数组
transformer = Binarizer(threshold=30).fit_transform(X)
transformer[:5]
```

##### 2. preprocessing.KBinsDiscretizer

**重要参数**

![004-连续型特征参数](D:\Machine_Learning\sklearn\3-数据预处理和特征工程\images\004-连续型特征参数.png)

```python
# preprocessing.KBinsDiscretizer

X = data.iloc[:,0].values.reshape(-1,1)
est = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')
est.fit_transform(X)

#查看转换后分的箱：变成了一列中的三箱
print(set(est.fit_transform(X).ravel()))

est = KBinsDiscretizer(n_bins=3, encode='onehot', strategy='uniform')
est.fit_transform(X).toarray()
```

<br>

## 3 特征选择 feature_selection

**特征提取(feature extraction)**

从文字，图像，声音等其他非结构化数据中提取新信息作为特征。比如说，从淘宝宝贝的名称中提取出产品类别，产品颜色，是否是网红产品等等。

**特征创造(feature creation)**

把现有特征进行组合，或互相计算，得到新的特征。比如说，我们有一列特征是速度，一列特征是距离，我们就可以通过让两列相处，创造新的特征：通过距离所花的时间。

**特征选择(feature selection)**
从所有的特征中，选择出有意义，对模型有帮助的特征，以避免必须将所有特征都导入模型去训练的情况。

### 3.1 Filter过滤法

#### 3.1.1 方差过滤

##### 3.1.1.1 VarianceThreshold

这是通过特征本身的方差来筛选特征的类。比如一个特征本身的方差很小，就表示样本在这个特征上基本没有差
异，可能特征中的大多数值都一样，甚至整个特征的取值都相同，那这个特征对于样本区分没有什么作用。所以无
论接下来的特征工程要做什么，都要**优先消除方差为0的特征**。VarianceThreshold有重要参数**threshold**，表示方差的**阈值**，表示舍弃所有方差小于threshold的特征，不填**默认为0**，即删除所有的记录都相同的特征。

```python
import numpy as np
import pandas as pd

data = pd.read_csv('./datas/digit recognizor.csv')
data.head()

X = data.iloc[:,1:]
y = data.iloc[:,0]
X.shape

#删除方差为0的数据
selector = VarianceThreshold() #实例化，不填参数默认方差为0
X_var0 = selector.fit_transform(X) #获取删除不合格特征之后的新特征矩阵

#也可以直接写成 X = VairanceThreshold().fit_transform(X)
X_var0.shape

#留下一半特征
X_fsvar = VarianceThreshold(np.median(X.var().values)).fit_transform(X) #通过中位数完成
X_fsvar.shape
```

当特征是二分类时，特征的取值就是伯努利随机变量，这些变量的方差可以计算为：
$$
V_{ar} = [X] = p(1-p)
$$
其中X是特征矩阵，p是二分类特征中的一类在这个特征中所占的概率。

```python
#若特征是伯努利随机变量，假设p=0.8，即二分类特征中某种分类占到80%以上的时候删除特征
X_bvar = VarianceThreshold(.8 * (1 - .8)).fit_transform(X)
X_bvar.shape
```

##### 3.1.1.2 方差过滤对模型的影响

##### 首先:

**随机森林的准确率略逊于KNN，但运行时间却连KNN的1%都不到，只需要十几秒钟。**

方差过滤后，随机森林的准确率也微弱上升，但运行时间却几乎是没什么变化，依然是11秒钟。为什么随机森林运行如此之快？为什么方差过滤对随机森林没很大的有影响？这是由于两种算法的原理中涉及到的计算量不同。最近邻算法KNN，单棵决策树，支持向量机SVM，神经网络，回归算法，都需要**遍历特征或升维**来进行运算，所以他们本身的运算量就很大，需要的时间就很长，因此**方差过滤这样的特征选择对他们来说就尤为重要**。但**对于不需要遍历特征的算法，比如随机森林，它随机选取特征进行分枝，本身运算就非常快速，因此特征选择对它来说效果平平**。这其实很容易理解，无论过滤法如何降低特征的数量，**随机森林也只会选取固定数量的特征来建模**；而最近邻算法就不同了，特征越少，距离计算的维度就越少，模型明显会随着特征的减少变得轻量。因此，过滤法的**主要对象**是：**需要遍历特征或升维的算法们**，而过滤法的**主要目的**是：**在维持算法表现的前提下，帮助算法们降低计算成本**。

##### 其次:

**过滤法对随机森林无效，却对树模型有效。**

从算法原理上来说，传统决策树需要遍历所有特征，计算不纯度后进行分枝，而随机森林却是随机选择特征进
行计算和分枝，因此随机森林的运算更快，过滤法对随机森林无用，对决策树却有用在sklearn中，决策树和随机森林都是随机选择特征进行分枝，但**决策树在建模过程中随机抽取的特征数目却远远超过随机森林当中每棵树随机抽取的特征数目**（比如说对于这个780维的数据，随机森林每棵树只会抽取10~20个特征，而决策树可能会抽取
300~400个特征），因此，过滤法对随机森林无用，却对决策树有用也因此，在sklearn中，随机森林中的每棵树都比单独的一棵决策树简单得多，高维数据下的随机森林的计算决策树快很多。

##### 3.1.1.3 选取超参数threshold

我们怎样知道，方差过滤掉的到底时噪音还是有效特征呢？过滤后模型到底会变好还是会变坏呢？答案是：每个数
据集不一样，只能自己去尝试。这里的方差阈值，其实相当于是一个超参数，要选定最优的超参数，我们可以画学
习曲线，找模型效果最好的点。但现实中，我们往往不会这样去做，因为这样会耗费大量的时间。我们只会**使用阈**
**值为0或者阈值很小的方差过滤**，来为我们优先消除一些明显用不到的特征，然后我们会选择更优的特征选择方法
继续削减特征数量。

#### 3.1.2 相关性过滤

##### 3.1.2.1 卡方过滤

卡方过滤是专门针对**离散型标签（即分类问题）**的相关性过滤。卡方检验类feature_selection.chi2计算每个非负
特征和标签之间的卡方统计量，并依照卡方统计量由高到低为特征排名。再结合feature_selection.SelectKBest
这个可以输入”评分标准“来选出前K个分数最高的特征的类，我们可以借此除去最可能独立于标签，与我们分类目
的无关的特征。
另外，如果卡方检验检测到某个特征中所有的值都相同，会提示我们使用方差先进行方差过滤。并且，刚才我们已
经验证过，当我们使用方差过滤筛选掉一半的特征后，模型的表现时提升的。因此在这里，我们使用threshold=中位数时完成的方差过滤的数据来做卡方检验（如果方差过滤后模型的表现反而降低了，那我们就不会使用方差过滤后的数据，而是使用原数据）：

```python
from sklearn.ensemble import RandomForestClassifier as RFC
from sklearn.model_selection import cross_val_score
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2

# 卡方过滤
#假设在这里我一直我需要300个特征
X_fschi = SelectKBest(chi2, k=300).fit_transform(X_fsvar, y)
print(X_fschi.shape)

print(cross_val_score(RFC(n_estimators=10,random_state=0),X_fschi,y,cv=5).mean())
```

可以看出，**模型的效果降低**了，这说明我们在设定k=300的时候删除了与模型相关且有效的特征，我们的**K值设置**
**得太小**，要么我们需要调整K值，要么我们必须放弃相关性过滤。当然，如果**模型的表现提升**，则说明我们的相关
性**过滤是有效的**，是过滤掉了模型的噪音的，这时候我们就保留相关性过滤的结果。

##### 3.1.2.2 选取超参数k

![005-卡方分布参数k](D:\Machine_Learning\sklearn\3-数据预处理和特征工程\images\005-卡方分布参数k.png)

```python
chivalue, pvalues_chi = chi2(X_fsvar,y)
print(chivalue) #卡方值
print()
print(pvalues_chi) #p
print()
print(chivalue.shape[0] - (pvalues_chi > 0.05).sum()) #差异性大于0.05的特征个数
#X_fschi = SelectKBest(chi2, k=填写具体的k).fit_transform(X_fsvar, y)
#cross_val_score(RFC(n_estimators=10,random_state=0),X_fschi,y,cv=5).mean()
```

**从特征工程的角度，我们希望选取卡方值很大，p值小于0.05的特征，即和标签是相关联的特征。**

##### 3.1.2.3 F检验

F检验，又称ANOVA，方差齐性检验，是用来捕捉每个特征与标签之间的线性关系的过滤方法。它即可以做回归也
可以做分类，因此包含**feature_selection.f_classif（F检验分类）**和**feature_selection.f_regression（F检验回**
**归）**两个类。其中F检验**分类**用于标签是**离散型变量**的数据，而F检验**回归**用于标签是**连续型变量**的数据。

和卡方检验一样，**这两个类需要和类SelectKBest连用**，并且我们也可以直接通过输出的统计量来判断我们到底要
设置一个什么样的K。需要注意的是，F检验在数据服从正态分布时效果会非常稳定，因此如果使用F检验过滤，我
们会先将数据转换成服从正态分布的方式。

F检验的本质是寻找两组数据之间的线性关系，其原假设是”数据不存在显著的线性关系“。它返回F值和p值两个统
计量。和卡方过滤一样，**我们希望选取p值小于0.05或0.01的特征，这些特征与标签时显著线性相关的**，而p值大于0.05或0.01的特征则被我们认为是和标签没有显著线性关系的特征，应该被删除。以F检验的分类为例，我们继续在数字数据集上来进行特征选择:

```python
from sklearn.feature_selection import f_classif

#K过滤
F, pvalues_f = f_classif(X_fsvar,y)

print(F) #F值
print()
print(pvalues_f) #p值
print()
print(F.shape[0] - (pvalues_f > 0.05).sum()) #k值大于0.05的特征个数

#X_fsF = SelectKBest(f_classif, k=填写具体的k).fit_transform(X_fsvar, y)
#cross_val_score(RFC(n_estimators=10,random_state=0),X_fsF,y,cv=5).mean()
```

该数据得到的结论和我们用卡方过滤得到的结论一模一样：**没有任何特征的p值大于0.01或者0.05**，所有的特征都是和标签相关的，因此我们不需要相关性过滤。

##### 3.1.2.4 互信息法

互信息法是用来捕捉每个特征与标签之间的任意关系（包括线性和非线性关系）的过滤方法。和F检验相似，它既
可以做回归也可以做分类，并且包含两个类f**eature_selection.mutual_info_classif（互信息分类）**和
**feature_selection.mutual_info_regression（互信息回归）**。这两个类的用法和参数都和F检验一模一样，不过互信息法比F检验更加强大，**F检验只能够找出线性关系，而互信息法可以找出任意关系**。互信息法不返回p值或F值类似的统计量，它返回“每个特征与目标之间的互信息量的估计”，这个估计量在[0,1]之间取值，**为0则表示两个变量独立，为1则表示两个变量完全相关**。以互信息分类为例的代码如下：

```python
from sklearn.feature_selection import mutual_info_classif as MIC

# 互信息法
result = MIC(X_fsvar,y)

k = result.shape[0] - sum(result <= 0)
print(k)

#X_fsF = SelectKBest(f_classif, k=填写具体的k).fit_transform(X_fsvar, y)
#cross_val_score(RFC(n_estimators=10,random_state=0),X_fsF,y,cv=5).mean()
```

**所有特征的互信息量估计都大于0，因此所有特征都与标签相关**。

##### 3.1.2.5过滤法 总结

![006-过滤法总结](D:\Machine_Learning\sklearn\3-数据预处理和特征工程\images\006-过滤法总结.png)

### 3.2 Embedded嵌入法

嵌入法是一种让算法自己决定使用哪些特征的方法，即特征选择和算法训练同时进行。在使用嵌入法时，我们先使
用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据权值系数从大到小选择特征。这些权值系
数往往代表了特征对于模型的某种贡献或某种重要性，比如决策树和树的集成模型中的feature_importances_属
性，可以列出各个特征对树的建立的贡献，我们就可以基于这种贡献的评估，找出对模型建立最有用的特征。因此
相比于过滤法，嵌入法的结果会更加精确到模型的效用本身，对于提高模型效力有更好的效果。并且，由于考虑特
征对模型的贡献，因此无关的特征（需要相关性过滤的特征）和无区分度的特征（需要方差过滤的特征）都会因为
缺乏对模型的贡献而被删除掉，可谓是过滤法的进化版。

另外，嵌入法引入了算法来挑选特征，因此其计算速度也会和应用的算法有很大的关系。如果采用计算量很大，计
算缓慢的算法，嵌入法本身也会非常耗时耗力。并且，在选择完毕之后，我们还是需要自己来评估模型。

##### feature_selection.SelectFromModel

estimator：使用的模型评估器，只要是带feature_importances_或者coef_属性，或带有l1和l2惩罚
项的模型都可以使用
threshold：特征重要性的阈值，重要性低于这个阈值的特征都将被删除

```python
from sklearn.feature_selection import SelectFromModel
from sklearn.ensemble import RandomForestClassifier as RFC

RFC_ = RFC(n_estimators =10,random_state=0)
X_embedded = SelectFromModel(RFC_,threshold=0.005).fit_transform(X,y)
#在这里我只想取出来有限的特征。0.005这个阈值对于有780个特征的数据来说，是非常高的阈值，因为平均每个特征
只能够分到大约0.001的feature_importances_
X_embedded.shape
#模型的维度明显被降低了

#同样的，我们也可以画学习曲线来找最佳阈值
#======【TIME WARNING：10 mins】======#
import numpy as np
import matplotlib.pyplot as plt
RFC_.fit(X,y).feature_importances_
threshold = np.linspace(0,(RFC_.fit(X,y).feature_importances_).max(),20)
score = []
for i in threshold:
    X_embedded = SelectFromModel(RFC_,threshold=i).fit_transform(X,y)
    once = cross_val_score(RFC_,X_embedded,y,cv=5).mean()
    score.append(once)
plt.plot(threshold,score)
plt.show()
```

### 3.3 Wrapper包装法

包装法也是一个特征选择和算法训练同时进行的方法，与嵌入法十分相似，它也是依赖于算法自身的选择，比如
coef_属性或feature_importances_属性来完成特征选择。但不同的是，我们往往使用一个目标函数作为黑盒来帮
助我们选取特征，而不是自己输入某个评估指标或统计量的阈值。包装法在初始特征集上训练评估器，并且通过
coef_属性或通过feature_importances_属性获得每个特征的重要性。然后，从当前的一组特征中修剪最不重要的
特征。在修剪的集合上递归地重复该过程，直到最终到达所需数量的要选择的特征。区别于过滤法和嵌入法的一次
训练解决所有问题，包装法要使用特征子集进行多次训练，因此它所需要的计算成本是最高的。

##### feature_selection.RFE

参数estimator是需要填写的实例化后的评估器，n_features_to_select是想要选择的特征个数，step表示每次迭
代中希望移除的特征个数。除此之外，RFE类有两个很重要的属性，.support_：返回所有的特征的是否最后被选
中的布尔矩阵，以及.ranking_返回特征的按数次迭代中综合重要性的排名。类feature_selection.RFECV会在交叉
验证循环中执行RFE以找到最佳数量的特征，增加参数cv，其他用法都和RFE一模一样。

```python
from sklearn.feature_selection import RFE
RFC_ = RFC(n_estimators =10,random_state=0)
selector = RFE(RFC_, n_features_to_select=340, step=50).fit(X, y)
selector.support_.sum()
selector.ranking_
X_wrapper = selector.transform(X)
cross_val_score(RFC_,X_wrapper,y,cv=5).mean()

#画学习曲线
#======【TIME WARNING: 15 mins】======#
score = []
for i in range(1,751,50):
    X_wrapper = RFE(RFC_,n_features_to_select=i, step=50).fit_transform(X,y)
    once = cross_val_score(RFC_,X_wrapper,y,cv=5).mean()
    score.append(once)
plt.figure(figsize=[20,5])
plt.plot(range(1,751,50),score)
plt.xticks(range(1,751,50))
plt.show()

```

注意，这个算法，指的不是我们最终用来导入数据的分类或回归算法（即不是随机森林），而是专业的
数据挖掘算法，即我们的目标函数。这些数据挖掘算法的核心功能就是选取最佳特征子集。最典型的目标函数是递归特征消除法（Recursive feature elimination, 简写为RFE）。它是一种贪婪的优化算法，
旨在找到性能最佳的特征子集。 它反复创建模型，并在每次迭代时保留最佳特征或剔除最差特征，下一次迭代时，它会使用上一次建模中没有被选中的特征来构建下一个模型，直到所有特征都耗尽为止。 然后，它根据自己保留或剔除特征的顺序来对特征进行排名，最终选出一个最佳子集。包装法的效果是所有特征选择方法中最利于提升模型表现的，它可以使用很少的特征达到很优秀的效果。除此之外，在特征数目相同时，包装法和嵌入法的效果能够匹敌，不过它比嵌入法算得更见缓慢，所以也不适用于太大型的数据。相比之下，包装法是最能保证模型效果的特征选择方法。