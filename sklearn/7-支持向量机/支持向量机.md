## 1 概述

支持向量机（SVM，也称为支持向量网络），是机器学习中获得关注最多的算法没有之一。它源于统计学习理论，是**集成算法，也是强学习器**。从学术的角度来看，**SVM是最接近深度学习的机器学习算法**。线性SVM可以看成是神经网络的单个神经元（虽然损失函数与神经网络不同），非线性的SVM则与两层的神经网络相当，非线性的SVM中如果添加多个核函数，则可以模仿多层的神经网络。

### 1.1 支持向量机分类器是如何工作的

#### 1.1.1 决策边界的确认

支持向量机所作的事情其实非常容易理解。先来看看下面这一组数据的分布，这是一组两种标签的数据，两种标签
分别由圆和方块代表。支持向量机的分类方法，是在这组分布中找出一个**超平面作为决策边界**，使模型在数据上的
**分类误差尽量小**，尤其是在未知数据集上的分类误差（泛化误差）尽量小。

![001-决策边界原图](D:\Machine_Learning\sklearn\7-支持向量机\images\001-决策边界原图.png)

##### 超平面

在几何中，超平面是一个空间的子空间，它是**维度比所在空间小一维的空间**。 如果数据空间本身是三维的，则其超平面是二维平面，而如果数据空间本身是二维的，则其超平面是一维的直线。在二分类问题中，如果一个超平面能够将数据划分为两个集合，其中每个集合中包含单独的一个类别，我们就说这个超平面是数据的“决策边界“。

#### 1.1.2 决策边界在训练集的划分

**但是，对于一个数据集来说，让训练误差为0的决策边界可以有无数条。**

但在此基础上，我们无法保证这条决策边界在未知数据集（测试集）上的表现也会优秀。对于现有的数据集来说，
**我们有B1和B2两条可能的决策边界**。我们可以把决策边界 向两边平移，直到碰到离这条决策边界最近的方块和圆圈后停下，形成两个新的超平面，分别是b11和b12，并且我们将原始的决策边界移动到b11和b12的中间，确保B1到 b11和b12的距离相等。在b11和b12中间的距离，叫做B1这条决策边界的边际(margin)，通常记作d。

为了简便，我们称**b11和b12为“虚线超平面”**，在其他博客或教材中可能有着其他的称呼，但大家知道是这两个超平面是由原来的决策边界向两边移动，直到**碰到距离原来的决策边界最近的样本后停下**而形成的超平面就可以了。

对B2也执行同样的操作，然后我们来对比一下两个决策边界。现在两条决策边界右边的数据都被判断为圆，左边
的数据都被判断为方块，两条决策边界在现在的数据集上的训练误差都是0，没有一个样本被分错。

![002-决策边界训练集](D:\Machine_Learning\sklearn\7-支持向量机\images\002-决策边界训练集.png)

#### 1.1.3 决策边界在测试集的划分

我们引入和原本的数据集相同分布的测试样本（红色所示），平面中的样本变多了，此时我们可以发现，对于B1而言，依然没有一个样本被分错，这条决策边界上的泛化误差也是0。但是对于B2而言，却有三个方块被误人类成
了圆，而有两个圆被误分类成了方块，这条决策边界上的泛化误差就远远大于B1了。这个例子表现出，**拥有更大**
**边际的决策边界在分类中的泛化误差更小**，这一点可以由结构风险最小化定律来证明（SRM）。如果边际很小，则任何轻微扰动都会对决策边界的分类产生很大的影响。**边际很小的情况，是一种模型在训练集上表现很好，却在测试集上表现糟糕的情况，所以会“过拟合”**。所以我们在找寻决策边界的时候，希望边际越大越好。

![003-决策边界测试集](D:\Machine_Learning\sklearn\7-支持向量机\images\003-决策边界测试集.png)

**支持向量机，就是通过找出边际最大的决策边界，来对数据进行分类的分类器**。也因此，支持向量分类器又叫做最
大边际分类器。这个过程在二维平面中看起来十分简单，但将上述过程使用数学表达出来，就不是一件简单的事情
了。

### 1.2 支持向量机原理的三层理解

和逻辑回归中的过程一样，SVM也是通过最小化损失函数来求解一个用于后续模型使用的重要信息：**决策边界**。

![004-支持向量机的三层理解](D:\Machine_Learning\sklearn\7-支持向量机\images\004-支持向量机的三层理解.png)

### 1.3 sklearn中的支持向量机

![005-sklearn中的支持向量机](D:\Machine_Learning\sklearn\7-支持向量机\images\005-sklearn中的支持向量机.png)

##### 注:

**1.**除了特别表明是线性的两个类**LinearSVC**和**LinearSVR**之外，其他的所有类都是同时支持线性和非线性的。

**2.**NuSVC和NuSVC**可以手动调节支持向量的数目**，其他参数都与最常用的SVC和SVR一致。
<br>

## 2 sklearn.svm.SVC

### 2.1 线性SVM用于分类的原理

#### 2.1.1 线性SVM的损失函数详解

https://www.bilibili.com/video/BV1WJ411k7L3?p=110

#### 2.1.2 函数间隔与几何间隔

https://www.bilibili.com/video/BV1WJ411k7L3?p=112

#### 2.1.3 线性SVM的拉格朗日对偶函数和决策函数

https://www.bilibili.com/video/BV1WJ411k7L3?p=113

#### 2.1.4 线性SVM决策过程的可视化

##### 画决策边界：理解函数contour

##### matplotlib.axes.Axes.contour([X, Y,] Z, [levels], **kwargs)

Contour是我们专门用来绘制等高线的函数。等高线，本质上是在二维图像上表现三维图像的一种形式，其中两维
X和Y是两条坐标轴上的取值，而Z表示高度。Contour就是将由X和Y构成平面上的所有点中，高度一致的点连接成
线段的函数，在同一条等高线上的点一定具有相同的Z值。我们可以利用这个性质来绘制我们的决策边界。

| 参数   | 含义                                                         |
| ------ | ------------------------------------------------------------ |
| X，Y   | 选填。两维平面上所有的点的横纵坐标取值，一般要求是二维结构并且形状需要与Z相同，往往通<br/>过numpy.meshgrid()这样的函数来创建。如果X和Y都是一维，则Z的结构必须为(len(Y), len(X))。<br/>如果不填写，则默认X = range(Z.shape[1])，Y = range(Z.shape[0])。 |
| Z      | 必填。平面上所有的点所对应的高度。                           |
| levels | 可不填，不填默认显示所有的等高线，填写用于确定等高线的数量和位置。如果填写整数n，则显<br/>示n个数据区间，即绘制n+1条等高线。水平高度自动选择。如果填写的是数组或列表，则在指定<br/>的高度级别绘制等高线。列表或数组中的值必须按递增顺序排列。 |



```python
# 1.导入需要的模块
import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False
from sklearn.datasets import make_blobs
from sklearn.svm import SVC
from sklearn.datasets import make_circles
from mpl_toolkits import mplot3d
from ipywidgets import interact,fixed

# 2.实例化数据集，可视化数据集
X,y = make_blobs(n_samples=50, centers=2, random_state=0,cluster_std=0.6) #方差为0.6
plt.scatter(X[:,0],X[:,1],c=y,s=50,cmap="rainbow") #c是标签序列
ax = plt.gca() #获取当前的子图，如果不存在，则创建新的子图
plt.xticks([])
plt.yticks([])
plt.show()

# 3.画决策边界：制作网格，理解函数meshgrid
#获取平面上两条坐标轴的最大值和最小值
xlim = ax.get_xlim()
ylim = ax.get_ylim()

#在最大值和最小值之间形成30个规律的数据
axisx = np.linspace(xlim[0],xlim[1],30)
axisy = np.linspace(ylim[0],ylim[1],30)

#从坐标向量中返回坐标矩阵
axisy,axisx = np.meshgrid(axisy,axisx)

#使用meshgrid函数将两个一维向量转换为特征矩阵
xy = np.vstack([axisx.ravel(), axisy.ravel()]).T

#xy就是已经形成的网格，它是遍布在整个画布上的密集的点
plt.scatter(xy[:,0],xy[:,1],s=1,cmap="rainbow")

'''
#理解函数meshgrid和vstack的作用
a = np.array([1,2,3])
b = np.array([7,8])
v1,v2 = np.meshgrid(a,b)

#两两组合，会得到多少个坐标？
#答案是6个，分别是 (1,7),(2,7),(3,7),(1,8),(2,8),(3,8)

v1 #array([[1, 2, 3],[1, 2, 3]])
v2 #array([[7, 7, 7],[8, 8, 8]])

v = np.vstack([v1.ravel(), v2.ravel()])
#array([[1, 2, 3, 1, 2, 3],
#       [7, 7, 7, 8, 8, 8]])
'''

# 4.建模，计算决策边界并找出网格上每个点到决策边界的距离
#建模，通过fit计算出对应的决策边界
clf = SVC(kernel = "linear").fit(X,y)
Z = clf.decision_function(xy).reshape(axisx.shape)
#重要接口decision_function，返回每个输入的样本所对应的到决策边界的距离
#然后再将这个距离转换为axisx的结构，这是由于画图的函数contour要求Z的结构必须与X和Y保持一致
#画决策边界和平行于决策边界的超平面
ax.contour(axisx,axisy,Z
           ,colors="k"
           ,levels=[-1,0,1] #画三条等高线，分别是Z为-1，Z为0和Z为1的三条线
           ,alpha=0.5
           ,linestyles=["--","-","--"])
ax.set_xlim(xlim)
ax.set_ylim(ylim)

# 5.将绘图过程包装成函数
#将上述过程包装成函数：
def plot_svc_decision_function(model,ax=None):
    if ax is None:
        ax = plt.gca()
    xlim = ax.get_xlim()
    ylim = ax.get_ylim()
    
    x = np.linspace(xlim[0],xlim[1],30)
    y = np.linspace(ylim[0],ylim[1],30)
    Y,X = np.meshgrid(y,x)
    xy = np.vstack([X.ravel(), Y.ravel()]).T
    P = model.decision_function(xy).reshape(X.shape)
    
    ax.contour(X, Y, P,colors="k",levels=[-1,0,1],alpha=0.5,linestyles=["--","-","--"])
    ax.set_xlim(xlim)
    ax.set_ylim(ylim)
    
# 6.绘图
clf = SVC(kernel = "linear").fit(X,y)
plt.scatter(X[:,0],X[:,1],c=y,s=50,cmap="rainbow")
plot_svc_decision_function(clf)

# 7.探索模型
#根据决策边界，对X中的样本进行分类，返回的结构为n_samples
print(clf.predict(X)) #[1 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1 0 0 1 1 1 1 1 0 0 0 1 0 1 0 0 0 0 1 1 0 1 0 1 0 1 1 0 1 1 0 1 0]
print()

#返回给定测试数据和标签的平均准确度
print(clf.score(X,y)) #1.0
print()

#返回支持向量
print(clf.support_vectors_)
'''
[[0.44359863 3.11530945]
 [2.33812285 3.43116792]
 [2.06156753 1.96918596]]
'''
print()

#返回每个类中支持向量的个数
print(clf.n_support_) #[2 1]

# 8.推广到非线性情况
X,y = make_circles(100, factor=0.1, noise=.1)
print(X.shape)
print(y.shape)
plt.scatter(X[:,0],X[:,1],c=y,s=50,cmap="rainbow")
plt.show()
clf = SVC(kernel = "linear").fit(X,y)
plt.scatter(X[:,0],X[:,1],c=y,s=50,cmap="rainbow")
plot_svc_decision_function(clf)

# 9.为非线性数据增加维度并绘制3D图像
#定义一个由x计算出来的新维度r
r = np.exp(-(X**2).sum(1))
rlim = np.linspace(min(r),max(r),0.2)
from mpl_toolkits import mplot3d
#定义一个绘制三维图像的函数
#elev表示上下旋转的角度
#azim表示平行旋转的角度
def plot_3D(elev=30,azim=30,X=X,y=y):
    ax = plt.subplot(projection="3d")
    ax.scatter3D(X[:,0],X[:,1],r,c=y,s=50,cmap='rainbow')
    ax.view_init(elev=elev,azim=azim)
    ax.set_xlabel("x")
    ax.set_ylabel("y")
    ax.set_zlabel("r")
    plt.show()
    
plot_3D()

# 10.将上述过程具体运行
X,y = make_circles(100, factor=0.1, noise=.1)
plt.scatter(X[:,0],X[:,1],c=y,s=50,cmap="rainbow")

def plot_svc_decision_function(model,ax=None):
    if ax is None:
        ax = plt.gca()
    xlim = ax.get_xlim()
    ylim = ax.get_ylim()
    
    x = np.linspace(xlim[0],xlim[1],30)
    y = np.linspace(ylim[0],ylim[1],30)
    Y,X = np.meshgrid(y,x)
    xy = np.vstack([X.ravel(), Y.ravel()]).T
    P = model.decision_function(xy).reshape(X.shape)
    
    ax.contour(X, Y, P,colors="k",levels=[-1,0,1],alpha=0.5,linestyles=["--","-","--"])
    ax.set_xlim(xlim)
    ax.set_ylim(ylim)
clf = SVC(kernel = "linear").fit(X,y)
plt.scatter(X[:,0],X[:,1],c=y,s=50,cmap="rainbow")
plot_svc_decision_function(clf)
r = np.exp(-(X**2).sum(1))
rlim = np.linspace(min(r),max(r),0.2)

def plot_3D(elev=30,azim=30,X=X,y=y):
    ax = plt.subplot(projection="3d")
    ax.scatter3D(X[:,0],X[:,1],r,c=y,s=50,cmap='rainbow')
    ax.view_init(elev=elev,azim=azim)
    ax.set_xlabel("x")
    ax.set_ylabel("y")
    ax.set_zlabel("r")
    plt.show()

interact(plot_3D,elev=[0,30],azip=(-180,180),X=fixed(X),y=fixed(y))
plt.show()
```

### 2.2 非线性SVM与核函数

#### 2.2.1 SVC在非线性数据上的推广

https://www.bilibili.com/video/BV1WJ411k7L3?p=123

#### 2.2.2 重要参数kernel

https://www.bilibili.com/video/BV1WJ411k7L3?p=124

![006-重要参数kernel](D:\Machine_Learning\sklearn\7-支持向量机\images\006-重要参数kernel.png)

#### 2.2.3 探索核函数在不同数据集上的表现

```python
# 1.导入所需要的库和模块
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
from sklearn import svm
from sklearn.datasets import make_circles, make_moons, make_blobs,make_classification

# 2.创建数据集，定义核函数的选择
n_samples = 100
datasets = [
    make_moons(n_samples=n_samples, noise=0.2, random_state=0),
    make_circles(n_samples=n_samples, noise=0.2, factor=0.5, random_state=1),
    make_blobs(n_samples=n_samples, centers=2, random_state=5),
    make_classification(n_samples=n_samples,n_features =
2,n_informative=2,n_redundant=0, random_state=5)
 ]
Kernel = ["linear","poly","rbf","sigmoid"]
#四个数据集分别是什么样子呢？
for X,Y in datasets:
    plt.figure(figsize=(5,4))
    plt.scatter(X[:,0],X[:,1],c=Y,s=50,cmap="rainbow")
    
# 3.构建子图并开始进行子图循环
#构建子图
nrows=len(datasets)
ncols=len(Kernel) + 1
fig, axes = plt.subplots(nrows, ncols,figsize=(20,16))

#第一层循环：在不同的数据集中循环
for ds_cnt, (X,Y) in enumerate(datasets):
    
    #在图像中的第一列，放置原数据的分布
    ax = axes[ds_cnt, 0]
    if ds_cnt == 0:
        ax.set_title("Input data")
    ax.scatter(X[:, 0], X[:, 1], c=Y, zorder=10, cmap=plt.cm.Paired,edgecolors='k')
    ax.set_xticks(())
    ax.set_yticks(())
    
    #第二层循环：在不同的核函数中循环
    #从图像的第二列开始，一个个填充分类结果
    for est_idx, kernel in enumerate(Kernel):
        
        #定义子图位置
        ax = axes[ds_cnt, est_idx + 1]
        
        #建模
        clf = svm.SVC(kernel=kernel, gamma=2).fit(X, Y)
        score = clf.score(X, Y)
        
        #绘制图像本身分布的散点图
        ax.scatter(X[:, 0], X[:, 1], c=Y
                   ,zorder=10
                   ,cmap=plt.cm.Paired,edgecolors='k')
        #绘制支持向量
        ax.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=50,
                    facecolors='none', zorder=10, edgecolors='k')
        
        #绘制决策边界
        x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5
        y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5
        
        #np.mgrid，合并了我们之前使用的np.linspace和np.meshgrid的用法
        #一次性使用最大值和最小值来生成网格
        #表示为[起始值：结束值：步长]
        #如果步长是复数，则其整数部分就是起始值和结束值之间创建的点的数量，并且结束值被包含在内
        XX, YY = np.mgrid[x_min:x_max:200j, y_min:y_max:200j]
        #np.c_，类似于np.vstack的功能
        Z = clf.decision_function(np.c_[XX.ravel(), YY.ravel()]).reshape(XX.shape)
        #填充等高线不同区域的颜色
        ax.pcolormesh(XX, YY, Z > 0, cmap=plt.cm.Paired)
        #绘制等高线
        ax.contour(XX, YY, Z, colors=['k', 'k', 'k'], linestyles=['--', '-', '--'],
                    levels=[-1, 0, 1])
        
        #设定坐标轴为不显示
        ax.set_xticks(())
        ax.set_yticks(())
        
        #将标题放在第一行的顶上
        if ds_cnt == 0:
            ax.set_title(kernel)
            
        #为每张图添加分类的分数  
        ax.text(0.95, 0.06, ('%.2f' % score).lstrip('0')
               , size=15
               , bbox=dict(boxstyle='round', alpha=0.8, facecolor='white')
               #为分数添加一个白色的格子作为底色
               , transform=ax.transAxes #确定文字所对应的坐标轴，就是ax子图的坐标轴本身
               , horizontalalignment='right' #位于坐标轴的什么方向
               )
plt.tight_layout()
plt.show()
```

##### 注:

**1.**可以观察到，线性核函数和多项式核函数在非线性数据上表现会浮动，如果数据相对线性可分，则表现不错，如果是像环形数据那样彻底不可分的，则表现糟糕。在线性数据集上，线性核函数和多项式核函数即便有扰动项也可以表现不错，可见多项式核函数是虽然也可以处理非线性情况，但更偏向于线性的功能。

**2.**Sigmoid核函数就比较尴尬了，它在非线性数据上强于两个线性核函数，但效果明显不如rbf，它在线性数据上完全比不上线性的核函数们，对扰动项的抵抗也比较弱，所以它功能比较弱小，很少被用到。

**3.**rbf，高斯径向基核函数基本在任何数据集上都表现不错，属于比较万能的核函数。我个人的经验是，无论如何先试试看高斯径向基核函数，它适用于核转换到很高的空间的情况，在各种情况下往往效果都很不错，如果rbf效果不好，那我们再试试看其他的核函数。另外，多项式核函数多被用于图像处理之中。

#### 2.2.4 探索核函数的优势和缺陷

```python
# 1.导入所需要的库和模块
from sklearn.datasets import load_breast_cancer
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import numpy as np
from time import time
import datetime
from sklearn.preprocessing import StandardScaler

# 2.绘制数据集
df = load_breast_cancer()
X = df.data
y = df.target
plt.scatter(X[:,0],X[:,1],c=y)
plt.show()

# 3.遍历核函数
X = StandardScaler().fit_transform(X) #统一量纲
Kernel = ["linear","poly","rbf","sigmoid"]
Xtrain, Xtest, Ytrain, Ytest = train_test_split(X,y,test_size=0.3,random_state=420)
for kernel in Kernel:
    time0 = time()
    clf= SVC(kernel = kernel
             , gamma="auto"
             , degree = 1
             , cache_size=5000
           ).fit(Xtrain,Ytrain)
    print("The accuracy under kernel %s is %f" % (kernel,clf.score(Xtest,Ytest)))
    print(datetime.datetime.fromtimestamp(time()-time0).strftime("%M:%S:%f"))
```

##### 注:

**1.线性核，尤其是多项式核函数在高次项时计算非常缓慢**

**2.rbf和多项式核函数都不擅长处理量纲不统一的数据集**

**3.SVM执行之前，最好先进行数据的无量纲化**

#### 2.2.5 选取与核函数相关的参数：degree & gamma & coef0

![007-选取与核函数相关的参数](D:\Machine_Learning\sklearn\7-支持向量机\images\007-选取与核函数相关的参数.png)

| 参数   | 含义                                                         |
| ------ | ------------------------------------------------------------ |
| degree | 整数，可不填，默认3<br/>多项式核函数的次数（'poly'），如果核函数没有选择"poly"，这个参数会被忽略 |
| gamma  | 浮点数，可不填，默认“auto"<br/>核函数的系数，仅在参数Kernel的选项为”rbf","poly"和"sigmoid”的时候有效<br/>输入“auto"，自动使用1/(n_features)作为gamma的取值<br/>输入"scale"，则使用1/(n_features * X.std())作为gamma的取值<br/>输入"auto_deprecated"，则表示没有传递明确的gamma值（不推荐使用） |
| coef0  | 浮点数，可不填，默认=0.0<br/>核函数中的常数项，它只在参数kernel为'poly'和'sigmoid'的时候有效 |

##### 2.2.5.1 rbf调参

对于高斯径向基核函数，调整gamma的方式其实比较容易，那就是画学习曲线

```python
score = []
gamma_range = np.logspace(-10, 1, 50) #返回在对数刻度上均匀间隔的数字
for i in gamma_range:
    clf = SVC(kernel="rbf",gamma = i,cache_size=5000).fit(Xtrain,Ytrain)
    score.append(clf.score(Xtest,Ytest))
    
print(max(score), gamma_range[score.index(max(score))])
plt.plot(gamma_range,score)
plt.show()
```

##### 2.2.5.2 poly调参

因为三个参数共同作用在一个数学公式上影响它的效果，因此使用**网格搜索**来共同调整三个参数

```python
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.model_selection import GridSearchCV
time0 = time()
gamma_range = np.logspace(-10,1,20)
coef0_range = np.linspace(0,5,10)
param_grid = dict(gamma = gamma_range
                 ,coef0 = coef0_range)
```

**注:一般使用线性核(liner)并调参或者高斯径向基(rbf)**

### 2.3 硬间隔与软间隔：重要参数C

#### 2.3.1 SVM在软间隔数据上的推广

![008-软间隔数据](D:\Machine_Learning\sklearn\7-支持向量机\images\008-软间隔数据.png)

**关键概念：硬间隔与软间隔**

当两组数据是**完全线性可分**，我们可以找出一个决策边界使得训练集上的分类误差为0，这两种数据就被称为是存在”**硬间隔**“。当两组数据**几乎是完全线性可分**的，但决策边界在训练集上存在较小的训练误差，这两种数据就被称为是存在”**软间隔**“。

#### 2.3.2 重要参数C

参数C用于权衡”训练样本的正确分类“与”决策函数的边际最大化“两个不可同时完成的目标，希望找出一个平衡点来让模型的效果最佳。

| 参数 | 含义                                                         |
| ---- | ------------------------------------------------------------ |
| C    | 浮点数，默认1，必须大于等于0，可不填<br/>松弛系数的惩罚项系数。如果C值设定比较大，那SVC可能会选择边际较小的，能够更好地分类所有训<br/>练点的决策边界，不过模型的训练时间也会更长。如果C的设定值较小，那SVC会尽量最大化边界，决<br/>策功能会更简单，但代价是训练的准确度。换句话说，C在SVM中的影响就像正则化参数对逻辑回归的<br/>影响 |

```python
#调线性核函数
score = []
C_range = np.linspace(0.01,30,50)
for i in C_range:
    clf = SVC(kernel="linear",C=i,cache_size=5000).fit(Xtrain,Ytrain)
    score.append(clf.score(Xtest,Ytest))
    
print(max(score), C_range[score.index(max(score))])
plt.plot(C_range,score)
plt.show()
#换rbf
score = []
C_range = np.linspace(0.01,30,50)
for i in C_range:
    clf = SVC(kernel="rbf",C=i,gamma =
0.012742749857031322,cache_size=5000).fit(Xtrain,Ytrain)
    score.append(clf.score(Xtest,Ytest))
    
print(max(score), C_range[score.index(max(score))])
plt.plot(C_range,score)
plt.show()
#进一步细化
score = []
C_range = np.linspace(5,7,50)
for i in C_range:
    clf = SVC(kernel="rbf",C=i,gamma =
0.012742749857031322,cache_size=5000).fit(Xtrain,Ytrain)
    score.append(clf.score(Xtest,Ytest))
    
print(max(score), C_range[score.index(max(score))])
plt.plot(C_range,score)
plt.show()
```

