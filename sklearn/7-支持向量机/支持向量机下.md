##  1 二分类SVC的进阶 

### 1.1 参数C的理解进阶 

![012-C进阶理解](D:\Machine_Learning\sklearn\7-支持向量机\images\012-C进阶理解.png)

此时我们的虚线超平面是由混杂在红色点中间的紫色点来决定的，所以此时此刻，这个紫色点 就是我们的支持向量了。所以**软间隔让决定两条虚线超平面的支持向量可能是来自于同一个类别的样本点，而硬间 隔的时候两条虚线超平面必须是由来自两个不同类别的支持向量决定的**。而C值会决定我们究竟是依赖红色点作为支持向量（只追求最大边界），还是我们要依赖软间隔中，混杂在红色点中的紫色点来作为支持向量（追求最大边界和判断正确的平衡）。

如果**C值设定比较大**，那SVC可能会选择边际较小的，**能够更好地分类所有训练点的决策边界**，不过**模型的训练时间也会更长**。如果**C的设定值较小**，那SVC会尽量最大化边界，尽量将掉落在决策边界另 一方的样本点预测正确，**决策功能会更简单**，但**代价是训练的准确度**，因为此时会有更多红色的点被分类错误。换句话说，**C在SVM中的影响就像正则化参数对逻辑回归的影响**。 此时此刻，所有可能影响我们的超平面的样本可能都会被定义为支持向量，所以支持向量就不再是所有压在虚线超平面上的点，而是所有可能影响我们的超平面的位置的那些混杂在彼此的类别中的点了。

### 1.2 二分类SVC中的样本不均衡问题：重要参数class_weight 

对于分类问题，永远都逃不过的一个痛点就是样本不均衡问题。  首先，**分类模型天生会倾向于多数的类，让多数类更容易被判断正确，少数类被牺牲掉**。因为对于模型而言，样本量越大的标签可以学习的信息越多，算法就会更加依赖于从多数类中学到的信息来进行判断。

比如，我们现在要对潜在犯罪者和普通人进行 分类，潜在犯罪者占总人口的比例是相当低的，也许只有2%左右，98%的人都是普通人，而我们的目标是要捕获出潜在犯罪者。这样的标签分布会带来许多问题。  如果我们**希望捕获少数类，模型就会失败**。其次，**模型评估指标会失去意义**。这种分类状况下，即便模型什么也不做，全**把所有人都当成不会犯罪的人，准确率也能非常高**，这使得模型评估指标accuracy变得毫无意义，根本无法达到我们的“要识别出会犯罪的人”的建模目的。 

在逻辑回归中已经介绍了一些基本方法，比如上采样下采样。但这些采样方法会增加样本的总数，对于支持向量机这个样本总是对计算速度影响巨大的算法来说，我们完全不想轻易地增加样本数量。况且，支**持向量机中地决策边界又仅仅受到参数C和支持向量的影响**，**单纯地增加样本数量不仅会增加计算时间**，可能**还会增加无数对决策边界无影响的样本点**。因此在支持向量机中，我们要大力依赖 我们调节样本均衡的参数：SVC类中的**class_weight**和接口fit中可以设定的**sample_weight**。

##### 1.2.1 SVC的参数：class_weight 

可输入**字典**或者"**balanced**”，可**不填**，**默认None**，对SVC，将类i的参数C设置为class_weight [i] * C。如果没有给出具体的class_weight，则所有类都被假设为占有相同的权重1，模型会根据数据原本的状况去训练。如果希望改善样本不均衡状况，请输入形如**{"标签的值1"：权重1，"标签的值2"：权重2}**的字典，则参数C将会自动被设为：**标签的值1的C：权重1 * C，标签的值2的C：权重2*C** 或者，可以使用“balanced”模式，这个模式使用y的值自动调整与输入数据中的类频率成反比的权重为 n_samples/(n_classes * np.bincount(y)) 

##### 1.2.2  SVC的接口fit的参数：sample_weight 

数组，结构为 (n_samples, )，必须对应输入fit中的特征矩阵的每个样本 每个样本在fit时的权重，让权重 * 每个样本对应的C值来迫使分类器强调设定的权重更大的样本。通常，较大的权重加在少数类的样本上，以迫使模型向着少数类的方向建模，通常来说，这两个参数我们只选取一个来设置。如果我们同时设置了两个参数，则C会同时受到两个参数的影响， 即 class_weight中设定的权重 * sample_weight中设定的权重 * C。

##### 1.2.3 二分类SVC中的样本不均衡问题解决

```python  
# 1.导入需要的库和模块
import numpy as np
import matplotlib.pyplot as plt
from sklearn import svm
from sklearn.datasets import make_blobs

# 2.创建样本不均衡的数据集
class_1 = 500 #类别1有500个样本
class_2 = 50 #类别2只有50个
centers = [[0.0, 0.0], [2.0, 2.0]] #设定两个类别的中心
clusters_std = [1.5, 0.5] #设定两个类别的方差，通常来说，样本量比较大的类别会更加松散
X, y = make_blobs(n_samples=550, centers=centers, cluster_std=clusters_std, random_state=0, shuffle=False)
#看看数据集长什么样
plt.scatter(X[:, 0], X[:, 1], c=y, cmap="rainbow",s=10) #其中红色点是少数类，紫色点是多数类

# 3.在数据集上分别建模
#不设定class_weight
clf = svm.SVC(kernel='linear', C=1.0)
clf.fit(X, y)

#设定class_weight
wclf = svm.SVC(kernel='linear', class_weight={1: 10})
wclf.fit(X, y)

#给两个模型分别打分,这个分数是accuracy准确度
print(clf.score(X,y)) #0.9345454545454546
print(wclf.score(X,y)) #0.9145454545454546

# 4.绘制两个模型下数据的决策边界
#首先要有数据分布
plt.figure(figsize=(6,5))
plt.scatter(X[:, 0], X[:, 1], c=y, cmap="rainbow",s=10)
ax = plt.gca() #获取当前的子图，如果不存在，则创建新的子图

#绘制决策边界的第一步：要有网格
xlim = ax.get_xlim()
ylim = ax.get_ylim()
xx = np.linspace(xlim[0], xlim[1], 30)
yy = np.linspace(ylim[0], ylim[1], 30)
YY, XX = np.meshgrid(yy, xx)
xy = np.vstack([XX.ravel(), YY.ravel()]).T
#第二步：找出我们的样本点到决策边界的距离
Z_clf = clf.decision_function(xy).reshape(XX.shape)
a = ax.contour(XX, YY, Z_clf, colors='black', levels=[0], alpha=0.5, linestyles=['-'])
Z_wclf = wclf.decision_function(xy).reshape(XX.shape)
b = ax.contour(XX, YY, Z_wclf, colors='red', levels=[0], alpha=0.5, linestyles=['-'])
#第三步：画图例
plt.legend([a.collections[0], b.collections[0]], ["non weighted", "weighted"],
           loc="upper right")
plt.show()
```

![013-class_weight](D:\Machine_Learning\sklearn\7-支持向量机\images\013-class_weight.png)

```python
print(clf.score(X,y)) #0.9345454545454546
print(wclf.score(X,y)) #0.9145454545454546
```

可以看出，从准确率的角度来看，**不做样本平衡的时候准确率反而更高，做了样本平衡准确率反而变低了**，这是因 为做了样本平衡后，为了要更有效地捕捉出少数类，模型误伤了许多多数类样本，而多数类被分错的样本数量大于少数类被分类正确的样本数量，使得模型整体的精确性下降。现在，如果我们的目的是模型整体的准确率，那我们就 要拒绝样本平衡，使用class_weight被设置之前的模型。

**然而在现实中，我们往往都在追求捕捉少数类，因为在很多情况下，将少数类判断错的代价是巨大的**。比如我们之 前提到的，判断潜在犯罪者和普通人的例子，如果我们没有能够识别出潜在犯罪者，那么这些人就可能去危害社 会，造成恶劣影响，但如果我们把普通人错认为是潜在犯罪者，我们也许只是需要增加一些监控和人为甄别的成 本。所以对我们来说，我们宁愿把普通人判错，也不想放过任何一个潜在犯罪者。我们希望不惜一切代价来捕获少 数类，或者希望捕捉出尽量多的少数类，那我们就必须使用class_weight设置后的模型。 

<br>

##  2 SVC的模型评估指标 

###  2.1 混淆矩阵（Confusion Matrix )

**混淆矩阵是二分类问题的多维衡量指标体系**，在样本不平衡时极其有用。在混淆矩阵中，我们**将少数类认为是正 例，多数类认为是负例**。在决策树，随机森林这些普通的分类算法里，即是说少数类是1，多数类是0。在SVM里， 就是说少数类是1，多数类是-1。普通的混淆矩阵，一般使用{0,1}来表示。混淆矩阵阵如其名，十分容易让人混 淆，在许多教材中，混淆矩阵中各种各样的名称和定义让人难以理解难以记忆。因此用简化的方式显示标准分类的混淆矩阵，如图所示： 

![014-混淆矩阵](D:\Machine_Learning\sklearn\7-支持向量机\images\014-混淆矩阵.png)

混淆矩阵中，永远是真实值在前，预测值在后。可以很容易看出，11和00的对角线就是全部预测正确的，01 和10的对角线就是全部预测错误的。基于混淆矩阵，我们有六个不同的模型评估指标，这些评估指标的范围都在 [0,1]之间，**所有以11和00为分子的指标都是越接近1越好**，**所有以01和10为分子的指标都是越接近0越好**。

#### 2.1.1 模型整体效果：准确率 

$$
Accuracy = \frac{11+00}{11+10+01+00}
$$

**准确率Accuracy**就是所有预测正确的所有样本除以总样本，通常来说越接近1越好 。

#### 2.1.2 捕捉少数类的艺术：精确度，召回率和F1 score 

##### 2.1.2.1 精确度

精确度Precision，又叫查准率，表示所有被我们预测为是少数类的样本中，**真正的少数类所占的比例**。在支持向 量机中，精确度可以被形象地表示为决策边界上方的所有点中，红色点所占的比例。精确度越高，代表我们捕捉正 确的红色点越多，对少数类的预测越精确。精确度越低，则代表我们误伤了过多的多数类。**精确度是”将多数类判错后所需付出成本“的衡量**。 
$$
Precision = \frac{11}{11+01}
$$

```python
# 5.准确率

#所有判断正确并确实为1的样本 / 所有被判断为1的样本
#对于没有class_weight，没有做样本平衡的灰色决策边界来说：
print((y[y == clf.predict(X)] == 1).sum()/(clf.predict(X) == 1).sum())   #0.8970099667774
      
#对于有class_weight，做了样本平衡的红色决策边界来说：
print((y[y == wclf.predict(X)] == 1).sum()/(wclf.predict(X) == 1).sum()) #0.8540372670807
```

注: 可以看出，**做了样本平衡之后，精确度是下降的**。因为很明显，样本平衡之后，有更多的多数类紫色点被我们误伤了。精确度可以帮助我们判断，是否每一次对少数类的预测都精确，所以又被称为”查准率“。在现实的样本不平衡例子中，**当每一次将多数类判断错误的成本非常高昂的时候（比如大众召回车辆的例子），我们会追求高精确度**。 精确度越低，我们对多数类的判断就会越错误。当然了，**如果我们的目标是不计一切代价捕获少数类，那我们并不在意精确度**。 

##### 2.1.2.2 召回率

召回率Recall，又被称为敏感度(sensitivity)，真正率，查全率，**表示所有真实为1的样本中，被我们预测正确的样 本所占的比例**。在支持向量机中，召回率可以被表示为，**决策边界上方的所有红色点占全部样本中的红色点的比 例**。召回率越高，代表我们尽量捕捉出了越多的少数类，召回率越低，代表我们没有捕捉出足够的少数类。 
$$
Recall = \frac{11}{11+10}
$$

```python
# 6.精确度

#所有predict为1的点 / 全部为1的点的比例
#对于没有class_weight，没有做样本平衡的灰色决策边界来说：
print((y[y == clf.predict(X)] == 1).sum()/(y == 1).sum()) #0.9818181818181818

#对于有class_weight，做了样本平衡的红色决策边界来说：
print((y[y == wclf.predict(X)] == 1).sum()/(y == 1).sum()) #1.0
```

可以看出，做样本平衡之前，我们只成功捕获了98%左右的少数类点，而做了样本平衡之后的模型，捕捉出100%的少数类点。召回率可以帮助我们判断，我们是否捕捉除了全部的少数类，所以又叫做查全率。 

**如果我们希望不计一切代价，找出少数类（比如找出潜在犯罪者的例子），那我们就会追求高召回率**，相反如果我 们的目标不是尽量捕获少数类，那我们就不需要在意召回率。 

注意召回率和精确度的分子是相同的（都是11），只是分母不同。**而召回率和精确度是此消彼长的，两者之间的平 衡代表了捕捉少数类的需求和尽量不要误伤多数类的需求的平衡**。究竟要偏向于哪一方，取决于我们的业务需求： 究竟是误伤多数类的成本更高，还是无法捕捉少数类的代价更高。 

##### 2.1.2.3  假负率 

**为了同时兼顾精确度和召回率**，我们**创造了两者的调和平均数**，作为考量两者平衡的综合性指标，并且将其称之为**F1 measure**。两个数之间的调和平均更加倾向于靠近两个数中比较小的那一个数，因此我们会不断的追求更高的F1 measure， 能够保证我们的精确度和召回率都比较高。**F1 measure在[0,1]之间分布，越接近1越好**。 Recall延申出来的另一个评估指标叫做假负率（False Negative Rate），它等于1 - Recall，用于衡量所有真实为1的样本中，被我们错误判断为0的，通常用得不多。  
$$
FNR = \frac{10}{11+10}
$$

####  2.1.3 判错多数类的考量：特异度与假正率 

##### 2.1.3.1 特异度

特异度(Specificity)表示所有真实为0的样本中，被正确预测为0的样本所占的比例。在支持向量机中，可以形象地 表示为，决策边界下方的点占所有紫色点的比例。  
$$
Specificity = \frac{00}{01+00}
$$

```python
#7.特异度

#所有被正确预测为0的样本 / 所有的0样本
#对于没有class_weight，没有做样本平衡的灰色决策边界来说：
print((y[y == clf.predict(X)] == 0).sum()/(y == 0).sum()) #0.8872727272727273

#对于有class_weight，做了样本平衡的红色决策边界来说：
print((y[y == wclf.predict(X)] == 0).sum()/(y == 0).sum()) #0.8290909090909091
```

 **特异度衡量了一个模型将多数类判断正确的能力** 

##### 2.1.3.2  假正率 

**1 - specificity**就**是一个模型将多数类判断错误的能力**，这种 能力被计算如下，并叫做假正率（False Positive Rate）。
$$
FPR = \frac{01}{01+00}
$$
在支持向量机中，**假正率就是决策边界上方的紫色点（所有被判断错误的多数类）占所有紫色点的比例**。根据我们 之前在precision处的分析，其实可以看得出来，**当样本均衡过后，假正率会更高，因为有更多紫色点被判断错误， 而样本均衡之前，假正率比较低，被判错的紫色点比较少**。所以假正率其实类似于Precision的反向指标， Precision衡量有多少少数点被判断正确，而假正率FPR衡量有多少多数点被判断错误，性质是十分类似的。 

#### 2.1.4 sklearn的混淆矩阵

| 类                                     | 含义                  |
| -------------------------------------- | --------------------- |
| sklearn.metrics.confusion_matrix       | 混淆矩阵              |
| sklearn.metrics.accuracy_score         | 准确率accuracy        |
| sklearn.metrics.precision_score        | 精确度precision       |
| sklearn.metrics.recall_score           | 召回率recall          |
| sklearn.metrics.precision_recall_curve | 精确度-召回率平衡曲线 |
| sklearn.metrics.f1_score               | F1 measure            |

###  2.2 ROC曲线以及其相关问题 

