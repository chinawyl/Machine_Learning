## 1 概述

### 1.1 名为“回归”的分类器

#### 1.1.1 线性回归概述

**逻辑回归，是一种名为“回归”的线性分类器，其本质是由线性回归变化而来的,线性公式如下**：
$$
z=\theta_0+\theta_1x_1+\theta_2x_2+...+\theta_nx_n=[\theta_0,\theta_1,\theta_2,...\theta_n]*
\left|
	\begin{matrix}
		x_{0} \\
		x_{1} \\
		x_{2} \\
		\vdots\\
		x_{n} \\
	\end{matrix}
\right|=
\theta^{T}x(x_0=1)
$$
线性回归的任务，就是构造一个预测函数 来映射输入的特征矩阵x和标签值y的线性关系，而**构造预测函数的核心**
**就是找出模型的参数**： θ^T和θ0 ，著名的最小二乘法就是用来求解线性回归中参数的数学方法。

#### 1.1.2 Sigmoid函数

通过函数 ，线性回归使用输入的特征矩阵X来输出一组连续型的标签值y_pred，以完成各种**预测连续型变量**的任务
（比如预测产品销量，预测股价等等）。那如果我们的标签是**离散型变量**，尤其是，如果是满足0-1分布的离散型
变量，我们要怎么办呢？我们可以通过引入联系函数(link function)，将线性回归方程z变换为g(z)，并且令g(z)的值分布在(0,1)之间，且当g(z)接近0时样本的标签为类别0，当g(z)接近1时样本的标签为类别1，这样就得到了一个分类模型。而这个联系函数对于逻辑回归来说，就是Sigmoid函数:
$$
g(z) = \frac{1}{1+e^{-z}}
$$
![001-Sigmoid函数](D:\Machine_Learning\sklearn\5-逻辑回归与评分卡\images\001-Sigmoid函数.jpg)

Sigmoid函数是一个S型的函数，当自变量z趋近正无穷时，因变量g(z)趋近于1，而当z趋近负无穷时，g(z)趋近
于0，它能够将任何实数映射到(0,1)区间，使其可用于将任意值函数转换为更适合二分类的函数。因为这个性质，Sigmoid函数也被当作是归一化的一种方法，与我们之前学过的MinMaxSclaer同理，是属于数据预处理中的“缩放”功能，可以将数据压缩到[0,1]之内。区别在于，**MinMaxScaler归一化之后，是可以取到0和1的（最大值归一化后就是1，最小值归一化后就是0），但Sigmoid函数只是无限趋近于0和1**。

#### 1.1.3 线性回归与逻辑回归关系

线性回归中**z=θ^Tx**，于是我们将z带入，就得到了二元逻辑回归模型的一般形式：
$$
g(z) = y(z) = \frac{1}{1+e^{-{\theta^T}x}}
$$
而y(x)就是我们逻辑回归返回的标签值。此时，y(x)的取值都在[0,1]之间，因此y(x)和1-y(x)相加必然为1。如
果我们令**y(x)除以1-y(x)可以得到形似几率**(odds)

不难发现，**y(x)的形似几率取对数的本质其实就是我们的线性回归z**，我们实际上是在对线性回归模型的预测结果取对数几率来让其的结果无限逼近0和1。因此，其对应的模型被称为”**对数几率回归**“（logistic Regression），也就是我们的逻辑回归，这个名为“回归”却是用来做分类工作的分类器。

### 1.2 为什么需要逻辑回归

##### 1.逻辑回归对线性关系的拟合效果好到丧心病狂

##### 2.逻辑回归计算快

##### 3.逻辑回归返回的分类结果不是固定的0，1，而是以小数形式呈现的类概率数字

##### 4.逻辑回归抗噪能力强。福布斯杂志在讨论逻辑回归的优点时，甚至有着“技术上来说，最佳模型的AUC面积低于0.8时，逻辑回归非常明显优于树模型”的说法。并且，逻辑回归在小数据集上表现更好，在大型的数据集上，树模型有着更好的表现。

由此，我们已经了解了逻辑回归的本质，它是一个返回对数几率的，在线性数据上表现优异的分类器，它主要被应
用在**金融领域**。其数学目的是求解能够让模型对数据拟合程度最高的参数 的值，以此构建预测函数 ，然后将
特征矩阵输入预测函数来计算出逻辑回归的结果y。

### 1.3 sklearn中的逻辑回归

| 逻辑回归相关的类                         | 说明                                               |
| ---------------------------------------- | -------------------------------------------------- |
| linear_model.LogisticRegression          | 逻辑回归分类器（又叫logit回归，最大熵分类器）      |
| linear_model.LogisticRegressionCV        | 带交叉验证的逻辑回归分类器                         |
| linear_model.SGDClassifier               | 计算Logistic回归模型以获得正则化参数的列表         |
| linear_model.SGDRegressor                | 利用梯度下降求解的线性分类器（SVM，逻辑回归等等）  |
| linear_model.SGDRegressor                | 利用梯度下降最小化正则化后的损失函数的线性回归模型 |
| metrics.log_loss 对数损失                | 又称逻辑损失或交叉熵损失                           |
|                                          |                                                    |
| 【在sklearn0.21版本中即将被移除】        |                                                    |
| inear_model.RandomizedLogisticRegression | 随机的逻辑回归                                     |
|                                          |                                                    |
| **其他会涉及的类**                       | **说明**                                           |
| metrics.confusion_matrix                 | 混淆矩阵，模型评估指标之一                         |
| metrics.roc_auc_score                    | ROC曲线，模型评估指标之一                          |
| metrics.accuracy_score                   | 精确性，模型评估指标之一                           |

<br>

## 2 linear_model.LogisticRegression

### 2.1 二元逻辑回归的损失函数

”损失函数“这个评估指标，来**衡量参数为 的模型拟合训练集时产生的信息损失的大小**，**并以此衡量参数θ的优劣**。如果用一组参数建模后，模型在训练集上表现良好，那我们就说模型拟合过程中的损失很小，损失函数的值很小，这一组参数就优秀；相反，如果模型在训练集上表现糟糕，损失函数就会很大，模型就训练不足，效果较差，这一组参数也就比较差。即是说，我们在求解参数 时，追求损失函数最小，让模型在训练数据上的拟合效果最优，即预测准确率尽量靠近100%

##### 注:没有”求解参数“需求的模型没有损失函数，比如KNN，决策树

$$
J(\theta)=-\sum_{i-1}^{m}(y_i*log(y_0(x_i))+(1-y_i)*log(1-y_0(x_i)))
$$



其中，θ表示求解出来的一组参数，m是样本的个数，yi是样本i上真实的标签，yθ(xi)是样本i上，基于参数θ计算
出来的逻辑回归返回值，xi是样本i各个特征的取值。我们的目标，就是求解出使J(θ)最小的θ取值。注意，在逻辑
回归的本质函数y(x)里，特征矩阵x是自变量，参数是θ。但**在损失函数中，参数θ是损失函数的自变量，x和y都是已知的特征矩阵和标签，相当于是损失函数的参数**。不同的函数中，自变量和参数各有不同，因此大家需要在数学计算中，尤其是求导的时候避免混淆。

### 2.2 重要参数penalty & C

#### 2.2.1 正则化

正则化是用来**防止模型过拟合**的过程，常用的有L1正则化和L2正则化两种选项，分别通过在损失函数后加上参数向θ的L1范式和L2范式的倍数来实现。这个增加的范式，被称为“正则项”，也被称为"惩罚项"。损失函数改变，基
于损失函数的最优化来求解的参数取值必然改变，我们以此来调节模型拟合的程度。其中L1范式表现为参数向量中的每个**参数的绝对值之和**，L2范数表现为参数向量中的每个**参数的平方和的开方值**。
$$
J(\theta)_{L1}=C*J(\theta)+\sum_{j=1}^n|\theta_j|\\(j>=1)
$$

$$
J(\theta)_{L1}=C*J(\theta)+\sqrt{\sum_{j=1}^n(\theta_j)^2}\\(j>=1)
$$

其中J(θ)是我们之前提过的损失函数，C是用来控制正则化程度的超参数，n是方程中特征的总数，也是方程中参
数的总数，j代表每个参数。在这里，j要大于等于1，是因为我们的参数向量θ中，第一个参数是θ0，是我们的截
距，它通常是不参与正则化的。

|    参数     | 说明                                                         |
| :---------: | ------------------------------------------------------------ |
| **penalty** | 可以输入"l1"或"l2"来指定使用哪一种正则化方式，**不填写默认"l2"**。注意，若选择"l1"正则化，参数solver仅能够使用求解方式”liblinear"和"saga“，若使用“l2”正则化，参数solver中所有的求解方式都可以使用 |
|    **C**    | C正则化强度的倒数，必须是一个大于0的浮点数，不填写默认1.0，即默认正则项与损失函数的比值是1：1。C越小，损失函数会越小，模型对损失函数的惩罚越重，正则化的效力越强，参数θ会逐渐被压缩得越来越小 |

L1正则化和L2正则化虽然都可以控制过拟合，但它们的效果并不相同。当正则化强度逐渐增大（即C逐渐变小），
参数 的取值会逐渐变小，但**L1正则化会将参数压缩为0，L2正则化只会让参数尽量小，不会取到0**。

```python
# 1.导入库和模块
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression as LR
from sklearn.datasets import load_breast_cancer
from sklearn.metrics import accuracy_score

# 2.导入数据并探索数据
data = load_breast_cancer()
X = data.data
y = data.target
data.data.shape

# 3.L1和L2正则化差别
lrl1 = LR(penalty="l1",solver="liblinear",C=0.5,max_iter=1000)
lrl2 = LR(penalty="l2",solver="liblinear",C=0.5,max_iter=1000)

#逻辑回归的重要属性coef_，查看每个特征所对应的参数
lrl1 = lrl1.fit(X,y)
print(lrl1.coef_)
print((lrl1.coef_ != 0).sum(axis=1))
print()

lrl2 = lrl2.fit(X,y)
print(lrl2.coef_)
print((lrl2.coef_ != 0).sum(axis=1))

# 4.绘制L1,L2在训练集和测试集上的学习曲线
l1 = []
l2 = []
l1test = []
l2test = []

Xtrain, Xtest, Ytrain, Ytest = train_test_split(X,y,test_size=0.3,random_state=420)

for i in np.linspace(0.05,1,19): #在0.05到1之间取19个数
    lrl1 = LR(penalty="l1",solver="liblinear",C=i,max_iter=1000)
    lrl2 = LR(penalty="l2",solver="liblinear",C=i,max_iter=1000)
    
    lrl1 = lrl1.fit(Xtrain,Ytrain)
    l1.append(accuracy_score(lrl1.predict(Xtrain),Ytrain))
    l1test.append(accuracy_score(lrl1.predict(Xtest),Ytest))
    lrl2 = lrl2.fit(Xtrain,Ytrain)
    l2.append(accuracy_score(lrl2.predict(Xtrain),Ytrain))
    l2test.append(accuracy_score(lrl2.predict(Xtest),Ytest))
    
graph = [l1,l2,l1test,l2test]
color = ["green","black","lightgreen","gray"]
label = ["L1","L2","L1test","L2test"]

plt.figure(figsize=(6,6))
for i in range(len(graph)):
    plt.plot(np.linspace(0.05,1,19),graph[i],color[i],label=label[i])
plt.legend(loc=4) #图例的位置在哪里?4表示，右下角
plt.show()
```

##### 注:

1.**L1正则化本质是一个特征选择**的过程，掌管了参数的**“稀疏性”**。L1正则化越强，参数向量中就越多的参数为0，参数就越稀疏，选出来的特征就越少，以此来防止过拟合。

2.**L2正则化**在加强的过程中，会尽量让**每个特征对模型都有一些小的贡献**，但携带信息少，对模型贡献不大
的**特征的参数会非常接近于0**。

3.如果L2正则化后还是过拟合，模型在未知数据集上的效果表现很差，就可以考虑L1正则化。

#### 2.2.2 逻辑回归中的特征工程

##### 嵌入法embedded

```python
# 1.导入库和模块

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression as LR
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import cross_val_score
from sklearn.feature_selection import SelectFromModel

# 2.使用L1进行降维

data = load_breast_cancer()
LR_ = LR(solver="liblinear",C=0.9,random_state=420)

#查看原始特征维度与准确率
print(data.data.shape)
print(cross_val_score(LR_,data.data,data.target,cv=10).mean())
print()

#查看降维后的特征维度与准确率
X_embedded = SelectFromModel(LR_,norm_order=1).fit_transform(data.data,data.target)
print(X_embedded.shape)
print(cross_val_score(LR_,X_embedded,data.target,cv=10).mean())

# 3.使用SelectFromModel这个类中的参数threshold调节阈值降维（没啥意义）

fullx = []
fsx = []
threshold = np.linspace(0,abs((LR_.fit(data.data,data.target).coef_)).max(),20)
k=0

for i in threshold:
    X_embedded = SelectFromModel(LR_,threshold=i).fit_transform(data.data,data.target)
    fullx.append(cross_val_score(LR_,data.data,data.target,cv=5).mean())
    fsx.append(cross_val_score(LR_,X_embedded,data.target,cv=5).mean())
    print((threshold[k],X_embedded.shape[1]))
    k+=1
    
plt.figure(figsize=(20,5))
plt.plot(threshold,fullx,label="full")
plt.plot(threshold,fsx,label="feature selection")
plt.xticks(threshold)
plt.legend()
plt.show()

# 4.调逻辑回归的类LR_，通过画C的学习曲线来实现降维

#第一次C学习曲线
fullx = []
fsx = []
C=np.arange(0.01,10.01,0.5)

for i in C:
    LR_ = LR(solver="liblinear",C=i,random_state=420)
    fullx.append(cross_val_score(LR_,data.data,data.target,cv=10).mean())
    
    X_embedded = SelectFromModel(LR_,norm_order=1).fit_transform(data.data,data.target)
    fsx.append(cross_val_score(LR_,X_embedded,data.target,cv=10).mean())
    
print(max(fsx),C[fsx.index(max(fsx))])
plt.figure(figsize=(20,5))
plt.plot(C,fullx,label="full")
plt.plot(C,fsx,label="feature selection")
plt.xticks(C)
plt.legend()
plt.show()

#细化学习曲线
fullx = []
fsx = []
C=np.arange(6.05,7.05,0.005)

for i in C:
    LR_ = LR(solver="liblinear",C=i,random_state=420)
    
    fullx.append(cross_val_score(LR_,data.data,data.target,cv=10).mean())
    
    X_embedded = SelectFromModel(LR_,norm_order=1).fit_transform(data.data,data.target)
    fsx.append(cross_val_score(LR_,X_embedded,data.target,cv=10).mean())
    
print(max(fsx),C[fsx.index(max(fsx))])
plt.figure(figsize=(20,5))
plt.plot(C,fullx,label="full")
plt.plot(C,fsx,label="feature selection")
plt.xticks(C)
plt.legend()
plt.show()

#验证模型效果：降维之前
LR_ = LR(solver="liblinear",C=6.069999999999999,random_state=420)
print(cross_val_score(LR_,data.data,data.target,cv=10).mean())
print()

#验证模型效果：降维之后
LR_ = LR(solver="liblinear",C=6.069999999999999,random_state=420)
X_embedded = SelectFromModel(LR_,norm_order=1).fit_transform(data.data,data.target)
print(cross_val_score(LR_,X_embedded,data.target,cv=10).mean())
print(X_embedded.shape)
```

##### 系数累加法

在PCA中，我们通过绘制累积可解释方差贡献率曲线来选择超参数，在逻辑回归中我们可以使用系数coef_来这样做，并且我们选择特征个数的逻辑也是类似的：找出曲线由锐利变平滑的转折点，转折点之前被累加的特征都是我们需要的，转折点之后的我们都不需要。不过这种方法相对比较麻烦，因为我们要先对特征系数进行从大到小的排序，还要确保我们知道排序后的每个系数对应的原始特征的位置，才能够正确找出那些重要的特征。如果要使用这样的方法，不如直接使用嵌入法来得方便。

##### 包装法

包装法可以直接设定我们需要的特征个数，逻辑回归在现实中运用时，可能会有”需要5~8个变量”这种需
求，包装法此时就非常方便了。

### 2.3 梯度下降：重要参数max_iter

逻辑回归的数学目的是求解能够让模型最优化，拟合程度最好的参数θ的值，即求解能够让损失函数J(θ)最小化的
值。对于二元逻辑回归来说，有多种方法可以用来求解参数θ，最常见的有梯度下降法(Gradient Descent)，坐标下降法(Coordinate Descent)，牛顿法(Newton-Raphson method)等，其中又以梯度下降法最为著名。每种方法都涉及复杂的数学原理，但这些计算在执行的任务其实是类似的。

#### 2.3.1 什么是梯度

在逻辑回归中，我们的损失函数如下所示：
$$
J(\theta)=-\sum_{i-1}^{m}(y_i*log(y_0(x_i))+(1-y_i)*log(1-y_0(x_i)))
$$
我们对这个函数上的自变量θ求偏导，就可以得到梯度向量在第 组 的坐标点上的表示形式：
$$
\frac{\partial}{\partial\theta_j}J(\theta) = d_j = \sum_{i=1}^m(y_\theta(x_i)-y_i)x_{ij}
$$
**一个多元函数的梯度，是对其自变量求偏导的结果，不是对其参数求偏导的结果**。但是在逻辑回归的数学过程
中，损失函数的自变量刚好是逻辑回归的预测函数y(x)的参数，所以才造成了这种让人误解的，“对多元函数的
参数求偏导”的写法。务必记住，正确的做法是：**在多元函数(损失函数)上对自变量(逻辑回归的预测函数y(x)**
**的参数)求偏导**，求解梯度的方式，和逻辑回归本身的预测函数y(x)没有一丝联系。

#### 2.3.2 什么是步长

许多博客和教材在描述步长的时候，声称它是”梯度下降中每一步沿梯度的反方向前进的长度“，”沿着最陡峭最
易下山的位置走的那一步的长度“或者”梯度下降中每一步损失函数减小的量“，甚至有说，步长是二维平面著名
的求导三角形中的”斜边“或者“对边”的。**这些说法都是错误的！步长不是任何物理距离，它甚至不是梯度下降过程中任何距离的直接变化，它是梯度向量的大小 上的一个比例，影响着参数向量 每次迭代后改变的部分**。

#### 2.3.3 找些损失函数最小点

```python
l2 = []
l2test = []
Xtrain, Xtest, Ytrain, Ytest = train_test_split(X,y,test_size=0.3,random_state=420)

for i in np.arange(1,201,10):
    lrl2 = LR(penalty="l2",solver="liblinear",C=0.9,max_iter=i)
    lrl2 = lrl2.fit(Xtrain,Ytrain)
    l2.append(accuracy_score(lrl2.predict(Xtrain),Ytrain))
    l2test.append(accuracy_score(lrl2.predict(Xtest),Ytest))
    
graph = [l2,l2test]
color = ["black","gray"]
label = ["L2","L2test"]
    
plt.figure(figsize=(20,5))
for i in range(len(graph)):
    plt.plot(np.arange(1,201,10),graph[i],color[i],label=label[i])
plt.legend(loc=4)
plt.xticks(np.arange(1,201,10))
plt.show()
#我们可以使用属性.n_iter_来调用本次求解中真正实现的迭代次数
lr = LR(penalty="l2",solver="liblinear",C=0.9,max_iter=300).fit(Xtrain,Ytrain)
lr.n_iter_
```

##### 注:

当max_iter中限制的步数已经走完了，逻辑回归却还没有找到损失函数的最小值，参数 的值还没有被收敛，
sklearn就会弹出红色警告，这是在提醒我们：参数没有收敛，请增大max_iter中输入的数字。但我们不一定要听sklearn的。max_iter很大，意味着步长小，模型运行得会更加缓慢。**虽然我们在梯度下降中追求的是损失函数的最小值，但这也可能意味着我们的模型会过拟合（在训练集上表现得太好，在测试集上却不一定），因此，如果在max_iter报红条的情况下，模型的训练和预测效果都已经不错了，那我们就不需要再增大max_iter中的数目了**，毕竟一切都以模型的预测效果为基准——只要最终的预测效果好，运行又快，那就一切都好，无所谓是否报红色警告了。

### 2.4 二元回归与多元回归：重要参数solver & multi_class

之前我们对逻辑回归的讨论，都是针对二分类的逻辑回归展开，其实sklearn提供了多种可以使用逻辑回归处理多
分类问题的选项。比如说，我们可以把某种分类类型都看作1，其余的分类类型都为0值，和”数据预处理“中的二值
化的思维类似，这种方法被称为"一对多"(One-vs-rest)，简称OvR，在sklearn中表示为“ovr"。又或者，我们可以把好几个分类类型划为1，剩下的几个分类类型划为0值，这是一种”多对多“(Many-vs-Many)的方法，简称MvM，在sklearn中表示为"Multinominal"。每种方式都配合L1或L2正则项来使用。

在sklearn中，我们使用参数multi_class来告诉模型，我们的预测标签是什么样的类型。

#### solver

求解器，可以输入"liblinear","lbfgs","newton-cg","sag","saga"

#### multi_class

输入"ovr", "multinomial", "auto"来告知模型，我们要处理的分类问题的类型。**默认是"ovr"**。
**1.'ovr':**表示分类问题是二分类，或让模型使用"一对多"的形式来处理多分类问题。

**2.'multinomial'：**表示处理多分类问题，这种输入在参数solver是'liblinear'时不可用。

**3."auto"：**表示会根据数据的分类情况和其他参数来确定模型要处理的分类问题的类型。比如说，如果数据是二分类，或者solver的取值为"liblinear"，"auto"会默认选择"ovr"。反之，则会选择"nultinomial"。
**注意：默认值将在0.22版本中从"ovr"更改为"auto"。**

![002-multi_class参数](D:\Machine_Learning\sklearn\5-逻辑回归与评分卡\images\002-multi_class参数.png)

查看看鸢尾花数据集上，multinomial和ovr的结果：

```python
from sklearn.datasets import load_iris
iris = load_iris()

for multi_class in ('multinomial', 'ovr'):
    clf = LogisticRegression(solver='sag', max_iter=100, random_state=42,
                             multi_class=multi_class).fit(iris.data, iris.target)
#打印两种multi_class模式下的训练分数
#%的用法，用%来代替打印的字符串中，想由变量替换的部分。%.3f表示，保留三位小数的浮点数。%s表示，字符串。
#字符串后的%后使用元祖来容纳变量，字符串中有几个%，元祖中就需要有几个变量
    print("training score : %.3f (%s)" % (clf.score(iris.data, iris.target),multi_class))
```

### 2.5 样本不平衡与参数class_weight

样本不平衡是指在一组数据集中，**标签的一类天生占有很大的比例，或误分类的代价很高，即我们想要捕捉出某种**
**特定的分类的时候的状况**。什么情况下误分类的代价很高？例如，我们现在要对潜在犯罪者和普通人进行分类，如果没有能够识别出潜在犯罪者，那么这些人就可能去危害社会，造成犯罪，识别失败的代价会非常高，但如果，我们将普通人错误地识别成了潜在犯罪者，代价却相对较小。所以我们宁愿将普通人分类为潜在犯罪者后再人工甄别，但是却不愿将潜在犯罪者分类为普通人，有种"宁愿错杀不能放过"的感觉。

再比如说，在银行要判断“一个新客户是否会违约”，通常不违约的人vs违约的人会是99：1的比例，真正违约的人
其实是非常少的。这种分类状况下，即便模型什么也不做，全把所有人都当成不会违约的人，正确率也能有99%，
这使得模型评估指标变得毫无意义，根本无法达到我们的“要识别出会违约的人”的建模目的。

因此我们要使用参数class_weight对样本标签进行一定的均衡，**给少量的标签更多的权重，让模型更偏向少数类，**
**向捕获少数类的方向建模**。该参数默认None，此模式表示自动给与数据集中的所有标签相同的权重，即自动1：
1。当误分类的代价很高的时候，我们使用”balanced“模式，我们只是希望对标签进行均衡的时候，什么都不填就
可以解决样本不均衡问题。

我们有着处理样本不均衡的各种方法，其中主流的是**采样法**，是通过重复样本的方式来平衡标签，可以进行上采样（增加少数类的样本），比如SMOTE，或者**下采样**（减少多数类的样本）。**对于逻辑回归来说，上采样是最好的办法**。