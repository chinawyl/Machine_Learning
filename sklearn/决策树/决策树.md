## 1 概述

### 1.1定义

### 是一种<font color=#FF0000 >非参数的有监督学习方法</font>，它能够从一系列有特征和标签的数据中总结出决策规则，并用树状图的结构来呈现这些规则，以解决<font color=#FF0000 >分类和回归问题</font>。决策树算法容易理解，适用各种数据，在解决各种问题时都有良好表现，尤其是以<font color=#FF0000 >树模型</font>为核心的各种集成算法，在各个行业和领域都有广泛的应用

### 1.2节点

##### 根节点：没有进边，有出边。包含最初的，针对特征的提问，即<font color=#FF0000 >最初的问题所在的地方</font>

##### 中间节点：既有进边也有出边，进边只有一条，出边可以有很多条，即<font color=#FF0000 >得到结论前的每一个问题</font>

##### 叶子节点：有进边，没有出边，每个叶子节点都是一个类别标签，即<font color=#FF0000 >得到的每一个结论</font>

##### 注:子节点和父节点：在两个相连的节点中，更接近根节点的是父节点，另一个是子节点

### 1.3决策树算法核心

##### 1.如何从数据表中找出最佳节点和最佳分枝

##### 2.如何让决策树停止生长，防止过拟合

##### 注:

##### <font color=#FF0000 >过拟合</font>:为所建的机器学习模型或者是深度学习模型在训练样本中表现得过于优越(因为神经网络已经学到了很多有用没用的特征)，导致在验证数据集以及测试数据集中表现不佳(有的特征完全没用啊，完全就是为了降低loss而得出来的特征)。即<font color=#FF0000 >学到了很多没必要的特征</font>。

##### <font color=#FF0000 >欠拟合</font>:可能训练样本被提取的特征比较少，导致训练出来的模型不能很好地匹配，表现得很差，甚至样本本身都无法高效的识别。即<font color=#FF0000 >训练样本被提取的特征比较少</font>。

### 1.4sklearn中的决策树

```python
'''
tree.DecisionTreeClassifier 分类树
tree.DecisionTreeRegressor 回归树
tree.export_graphviz 将生成的决策树导出为DOT格式，画图专用
tree.ExtraTreeClassifier 高随机版本的分类树
tree.ExtraTreeRegressor 高随机版本的回归树
'''

from sklearn import tree #导入需要的模块
clf = tree.DecisionTreeClassifier()     #实例化
clf = clf.fit(X_train,y_train) #用训练集数据训练模型
result = clf.score(X_test,y_test) #导入测试集，从接口中调用需要的信息
```

## 2 DecisionTreeClassifier与红酒数据集(分类树)

### 2.1重要参数

#### 2.1.1 criterion

为了要将表格转化为一棵树，决策树需要找出最佳节点和最佳的分枝方法，对分类树来说，衡量这个“最佳”的指标
叫做<font color=#FF0000 >“不纯度”</font>。通常来说，<font color=#FF0000 >不纯度越低，决策树对训练集的拟合越好</font>。现在使用的决策树算法在分枝方法上的核心
大多是围绕在对某个不纯度相关指标的最优化上。不纯度基于节点来计算，树中的每个节点都会有一个不纯度，并且子节点的不纯度一定是低于父节点的，也就是说，在同一棵决策树上，叶子节点的不纯度一定是最低的。
Criterion这个参数正是用来决定不纯度的计算方法的。sklearn提供了两种选择：

##### 1.输入”entropy“，使用信息熵（Entropy）

##### 2.输入”gini“，使用基尼系数（Gini Impurity）

![001](D:\Machine_Learning\sklearn\决策树\001.png)

#### 注:

##### 1).信息熵信息论中的**信息量**和**信息熵。**

##### 信息量

信息量是对信息的度量，就跟温度的度量是摄氏度一样，信息的大小跟随机事件的概率有关。

例如： 在哈尔滨的冬天，一条消息说：哈尔滨明天温度30摄氏度，这个事件肯定会引起轰动，因为它发生的		概率很小（信息量大）。日过是夏天，“明天温度30摄氏度”可能没有人觉得是一个新闻，因为夏天温度30摄		氏度太正常了，概率太大了（信息点太小了）



从这个例子中可以看出 一个随机事件的信息量的大小与其发生概率是成**反相关**的。

一个事件的信息信息量为：**I(X) = log2(1/p)** 其中p为事件X发生的概率。

##### 信息熵

一个随机变量 X 可以代表n个随机事件，对应的随机变为X=xi,

那么熵的定义就是 X的加权信息量。

H(x) = p(x1)I(x1)+...+p(xn)I(xn) 

​        = p(x1)log2(1/p(x1)) +.....+p(xn)log2(1/p(xn))

​        = -p(x1)log2(p(x1)) - ........-p(xn)log2(p(xn))

其中p(xi)代表xi发生的概率



例如：有32个足球队比赛，每一个队的实力相当，那么每一个对胜出的概率都是1/32

那么 要猜对哪个足球队胜出 非常困难，

这个时候的熵H(x) = 32 * (1/32)log(1/(1/32)) = 5



熵也可以作为一个**系统的混乱程度**的标准

试想如果32个队中有一个是ac米兰，另外31个对是北邮计算机1班队，2班，...31班

那么几乎只有一个可能 ac米兰胜利的概率是100%，其他的都是0%，这个系统的熵

就是 1*log(1/1) = 0. **这个系统其实是有序的，熵很小，而前面熵为5 系统处于无序状态。**

##### 2)、基尼不纯度

**基尼不纯度的大概意思是 一个随机事件变成它的对立事件的概率**

例如：一个随机事件X ，P(X=0) = 0.5 ,P(X=1)=0.5

那么基尼不纯度就为：P(X=0)*(1 - P(X=0)) + P(X=1)*(1 - P(X=1))  = 0.5

一个随机事件Y，P(Y=0) = 0.1 ,P(Y=1)=0.9

那么基尼不纯度就为P(Y=0)*(1 - P(Y=0)) +   P(Y=1)*(1 - P(Y=1))  = 0.18

很明显 X比Y更混乱，因为两个都为0.5 很难判断哪个发生。而Y就确定得多，Y=0的基尼不纯度很大。

**所以基尼不纯度也可以作为 衡量系统混乱程度的标准**

##### 3).信息熵与基尼系数差别

比起基尼系数，信息熵对不纯度**更加敏感**，对不纯度的**惩罚最强**。但是**在实际使用中，信息熵和基尼系数的效果基**
**本相同**。信息熵的计算比基尼系数**缓慢**一些，因为基尼系数的计算不涉及对数。另外，因为信息熵对不纯度更加敏
感，所以信息熵作为指标时，决策树的生长会更加“精细”，因此**对于高维数据或者噪音很多的数据，信息熵很容易**
**过拟合**，基尼系数在这种情况下效果往往比较好。当**模型拟合程度不足的时候，即当模型在训练集和测试集上都表**
**现不太好的时候，使用信息熵**。**维度低，数据比较清晰的时候，信息熵和基尼系数没区别**。当然，这些不是绝对的。

##### 建立一棵树代码

```python
#1. 导入需要的模块
from sklearn import tree
from sklearn.datasets import load_wine
from sklearn.model_selection import train_test_split

#2. 探索数据
wine = load_wine()

wine.data #数据

wine.target #标签

wine.data.shape #数据结构

import pandas as pd
pd.concat([pd.DataFrame(wine.data),pd.DataFrame(wine.target)],axis=1) #将wine拼接为表

wine.feature_names #特征名字

wine.target_names #标签名字

#3. 分训练集和测试集
Xtrain, Xtest, Ytrain, Ytest = train_test_split(wine.data,wine.target,test_size=0.3)
Xtrain.shape #训练集
Xtest.shape #测试集

#4. 建立模型
clf = tree.DecisionTreeClassifier(criterion="entropy") #选择信息熵
clf = clf.fit(Xtrain, Ytrain)
score = clf.score(Xtest, Ytest) #返回预测的准确度

score

#5. 画出一棵树吧
feature_name = ['酒精','苹果酸','灰','灰的碱性','镁','总酚','类黄酮','非黄烷类酚类','花青素','颜色强度','色调','od280/od315稀释葡萄酒','脯氨酸']
dot_data = tree.export_graphviz(clf
                               ,out_file = None
                               ,feature_names= feature_name
                               ,class_names=["01","02","03"]
                               ,filled=True #颜色:True为有颜色,颜色越深，不纯度越低
                               ,rounded=True #框的形状:True为有圆角
                               )
graph = graphviz.Source(dot_data)

graph

#6. 探索决策树
clf.feature_importances_

[*zip(feature_name,clf.feature_importances_) #根节点贡献度是最高的
 
clf = tree.DecisionTreeClassifier(criterion="entropy",random_state=30) #随机数种子
clf = clf.fit(Xtrain, Ytrain)
score = clf.score(Xtest, Ytest) #返回预测的准确度
 
score
```

#### 2.1.2 random_state & splitter

random_state用来设置分枝中的随机模式的参数，默认None，在**高维度时随机性会表现更明显**，**低维度的数据**
**（比如鸢尾花数据集），随机性几乎不会显现**。输入任意整数，会一直长出同一棵树，让模型稳定下来。

splitter也是用来控制决策树中的随机选项的，有两种输入值，**输入”best"，决策树在分枝时虽然随机，但是还是会优先选择更重要的特征进行分枝**（重要性可以通过属性feature_importances_查看），**输入“random"，决策树在分枝时会更加随机**，树会因为含有更多的不必要信息而更深更大，并因这些不必要信息而降低对训练集的拟合。这也是防止过拟合的一种方式。当你预测到你的模型会过拟合，用这两个参数来帮助你降低树建成之后过拟合的可能性。当然，树一旦建成，我们依然是使用剪枝参数来防止过拟合。

##### 固定结果代码

```python
clf = tree.DecisionTreeClassifier(criterion="entropy"
                                 ,random_state=30
                                 ,splitter="random"
                                 )
clf = clf.fit(Xtrain, Ytrain)
score = clf.score(Xtest, Ytest)

score

import graphviz
dot_data = tree.export_graphviz(clf
                               ,feature_names= feature_name
                               ,class_names=["01","02","03"]
                               ,filled=True
                               ,rounded=True
                               )  
graph = graphviz.Source(dot_data)

graph
```

#### 2.1.3剪枝参数

##### max_depth

**限制树的最大深度，超过设定深度的树枝全部剪掉**
这是用得最广泛的剪枝参数，在高维度低样本量时非常有效。决策树多生长一层，对样本量的需求会增加一倍，所
以限制树深度能够有效地限制过拟合。在集成算法中也非常实用。实际使用时，**建议从=3开始尝试**，看看拟合的效果再决定是否增加设定深度。

##### min_samples_leaf & min_samples_split

min_samples_leaf限定，一个节点在分枝后的每个子节点都必须包含至少min_samples_leaf个训练样本，否则分
枝就不会发生，或者，分枝会朝着满足每个子节点都包含min_samples_leaf个样本的方向去发生。
一般搭配max_depth使用，在回归树中有神奇的效果，可以让模型变得更加平滑。这个参数的**数量设置得太小会引起过拟合，设置得太大就会阻止模型学习数据**。一般来说，**建议从=5开始使用**。如果叶节点中含有的样本量变化很大，建议输入浮点数作为样本量的百分比来使用。同时，这个参数可以保证每个叶子的最小尺寸，可以在回归问题中避免低方差，过拟合的叶子节点出现。对于类别不多的分类问题，=1通常就是最佳选择。

min_samples_split限定，一个节点必须要包含至少min_samples_split个训练样本，这个节点才允许被分枝，否则分枝就不会发生。

##### 剪枝参数代码

```python
clf = tree.DecisionTreeClassifier(criterion="entropy"
                                 ,random_state=30
                                 ,splitter="random"
                                 ,max_depth=3 #树的最大深度
                                 ,min_samples_leaf=10 #节点分支后最小样本满足数
                                 ,min_samples_split=10 #节点分支后最小样本满足数
                                 )
clf = clf.fit(Xtrain, Ytrain)

dot_data = tree.export_graphviz(clf
                               ,feature_names= feature_name
                               ,class_names=["琴酒","雪莉","贝尔摩德"]
                               ,filled=True
                               ,rounded=True
                               )  
graph = graphviz.Source(dot_data)

graph

clf.score(Xtrain,Ytrain)
clf.score(Xtest,Ytest)
```

##### 确认最优的剪枝参数代码

```python
import matplotlib.pyplot as plt
test = []
for i in range(10):
    clf = tree.DecisionTreeClassifier(max_depth=i+1
                                     ,criterion="entropy"
                                     ,random_state=30
                                     ,splitter="random"
                                     )
    clf = clf.fit(Xtrain, Ytrain)
    score = clf.score(Xtest, Ytest)
    test.append(score)
plt.plot(range(1,11),test,color="red",label="max_depth")
plt.legend()
plt.show()
```

#### 注:

无论如何，剪枝参数的默认值会让树无尽地生长，这些树在某些数据集上可能非常巨大，对内存的消耗也非常巨
大。所以如果你手中的数据集非常巨大，你已经预测到无论如何你都是要剪枝的，那提前设定这些参数来控制树的
复杂性和大小会比较好。

#### 2.1.4 目标权重参数

##### class_weight & min_weight_fraction_leaf

完成样本标签平衡的参数。样本不平衡是指在一组数据集中，标签的一类天生占有很大的比例。比如说，在银行要
判断“一个办了信用卡的人是否会违约”，就是是vs否（1%：99%）的比例。这种分类状况下，即便模型什么也不
做，全把结果预测成“否”，正确率也能有99%。因此我们要使用class_weight参数对样本标签进行一定的均衡，给
少量的标签更多的权重，让模型更偏向少数类，向捕获少数类的方向建模。该参数默认None，此模式表示自动给
与数据集中的所有标签相同的权重。
有了权重之后，样本量就不再是单纯地记录数目，而是受输入的权重影响了，因此这时候剪枝，就需要搭配min_
weight_fraction_leaf这个基于权重的剪枝参数来使用。另请注意，基于权重的剪枝参数（例如min_weight_
fraction_leaf）将比不知道样本权重的标准（比如min_samples_leaf）更少偏向主导类。如果样本是加权的，则使用基于权重的预修剪标准来更容易优化树结构，这确保叶节点至少包含样本权重的总和的一小部分。

### 2.2 重要属性和接口

属性是在模型训练之后，能够调用查看的模型的各种性质。对决策树来说，最重要的是feature_importances_，能够查看各个特征对模型的重要性。

sklearn中许多算法的接口都是相似的，比如说我们之前已经用到的fit和score，几乎对每个算法都可以使用。除了
这两个接口之外，决策树最常用的接口还有apply和predict。apply中输入测试集返回每个测试样本所在的叶子节
点的索引，predict输入测试集返回每个测试样本的标签。

在这里不得不提的是，所有接口中要求输入X_train和X_test的部分，**输入的特征矩阵必须至少是一个<font color=#FF0000 >二维矩阵</font>。**
**sklearn不接受任何一维矩阵作为特征矩阵被输入。**如果你的数据的确只有一个特征，那必须用reshape(-1,1)来给
矩阵增维；如果你的数据只有一个特征和一个样本，使用reshape(1,-1)来给你的数据增维。

#### 代码

```python
#apply返回每个测试样本所在的叶子节点的索引
clf.apply(Xtest)
#predict返回每个测试样本的分类/回归结果
clf.predict(Xtest)
```

#### 注:

决策树天生不擅长环形数据