## 1 概述

### 1.1定义

### 是一种<font color=#FF0000 >非参数的有监督学习方法</font>，它能够从一系列有特征和标签的数据中总结出决策规则，并用树状图的结构来呈现这些规则，以解决<font color=#FF0000 >分类和回归问题</font>。决策树算法容易理解，适用各种数据，在解决各种问题时都有良好表现，尤其是以<font color=#FF0000 >树模型</font>为核心的各种集成算法，在各个行业和领域都有广泛的应用

### 1.2节点

##### 根节点：没有进边，有出边。包含最初的，针对特征的提问，即<font color=#FF0000 >最初的问题所在的地方</font>

##### 中间节点：既有进边也有出边，进边只有一条，出边可以有很多条，即<font color=#FF0000 >得到结论前的每一个问题</font>

##### 叶子节点：有进边，没有出边，每个叶子节点都是一个类别标签，即<font color=#FF0000 >得到的每一个结论</font>

##### 注:子节点和父节点：在两个相连的节点中，更接近根节点的是父节点，另一个是子节点

### 1.3决策树算法核心

##### 1.如何从数据表中找出最佳节点和最佳分枝

##### 2.如何让决策树停止生长，防止过拟合

##### 注:

##### <font color=#FF0000 >过拟合</font>:为所建的机器学习模型或者是深度学习模型在训练样本中表现得过于优越(因为神经网络已经学到了很多有用没用的特征)，导致在验证数据集以及测试数据集中表现不佳(有的特征完全没用啊，完全就是为了降低loss而得出来的特征)。即<font color=#FF0000 >学到了很多没必要的特征</font>。

##### <font color=#FF0000 >欠拟合</font>:可能训练样本被提取的特征比较少，导致训练出来的模型不能很好地匹配，表现得很差，甚至样本本身都无法高效的识别。即<font color=#FF0000 >训练样本被提取的特征比较少</font>。

### 1.4sklearn中的决策树

```python
'''
tree.DecisionTreeClassifier 分类树
tree.DecisionTreeRegressor 回归树
tree.export_graphviz 将生成的决策树导出为DOT格式，画图专用
tree.ExtraTreeClassifier 高随机版本的分类树
tree.ExtraTreeRegressor 高随机版本的回归树
'''

from sklearn import tree #导入需要的模块
clf = tree.DecisionTreeClassifier()     #实例化
clf = clf.fit(X_train,y_train) #用训练集数据训练模型
result = clf.score(X_test,y_test) #导入测试集，从接口中调用需要的信息
```

## 2 DecisionTreeClassifier(分类树)与红酒数据集

### 2.1重要参数

#### 2.1.1 criterion

为了要将表格转化为一棵树，决策树需要找出最佳节点和最佳的分枝方法，对分类树来说，衡量这个“最佳”的指标
叫做<font color=#FF0000 >“不纯度”</font>。通常来说，<font color=#FF0000 >不纯度越低，决策树对训练集的拟合越好</font>。现在使用的决策树算法在分枝方法上的核心
大多是围绕在对某个不纯度相关指标的最优化上。不纯度基于节点来计算，树中的每个节点都会有一个不纯度，并且子节点的不纯度一定是低于父节点的，也就是说，在同一棵决策树上，叶子节点的不纯度一定是最低的。
Criterion这个参数正是用来决定不纯度的计算方法的。sklearn提供了两种选择：

##### 1.输入”entropy“，使用信息熵（Entropy）

##### 2.输入”gini“，使用基尼系数（Gini Impurity）

![001](D:\Machine_Learning\sklearn\决策树\001.png)

#### 注:

##### 1).信息熵信息论中的**信息量**和**信息熵。**

##### 信息量

信息量是对信息的度量，就跟温度的度量是摄氏度一样，信息的大小跟随机事件的概率有关。

例如： 在哈尔滨的冬天，一条消息说：哈尔滨明天温度30摄氏度，这个事件肯定会引起轰动，因为它发生的		概率很小（信息量大）。日过是夏天，“明天温度30摄氏度”可能没有人觉得是一个新闻，因为夏天温度30摄		氏度太正常了，概率太大了（信息点太小了）



从这个例子中可以看出 一个随机事件的信息量的大小与其发生概率是成**反相关**的。

一个事件的信息信息量为：**I(X) = log2(1/p)** 其中p为事件X发生的概率。

##### 信息熵

一个随机变量 X 可以代表n个随机事件，对应的随机变为X=xi,

那么熵的定义就是 X的加权信息量。

H(x) = p(x1)I(x1)+...+p(xn)I(xn) 

​        = p(x1)log2(1/p(x1)) +.....+p(xn)log2(1/p(xn))

​        = -p(x1)log2(p(x1)) - ........-p(xn)log2(p(xn))

其中p(xi)代表xi发生的概率



例如：有32个足球队比赛，每一个队的实力相当，那么每一个对胜出的概率都是1/32

那么 要猜对哪个足球队胜出 非常困难，

这个时候的熵H(x) = 32 * (1/32)log(1/(1/32)) = 5



熵也可以作为一个**系统的混乱程度**的标准

试想如果32个队中有一个是ac米兰，另外31个对是北邮计算机1班队，2班，...31班

那么几乎只有一个可能 ac米兰胜利的概率是100%，其他的都是0%，这个系统的熵

就是 1*log(1/1) = 0. **这个系统其实是有序的，熵很小，而前面熵为5 系统处于无序状态。**

##### 2)、基尼不纯度

**基尼不纯度的大概意思是 一个随机事件变成它的对立事件的概率**

例如：一个随机事件X ，P(X=0) = 0.5 ,P(X=1)=0.5

那么基尼不纯度就为：P(X=0)*(1 - P(X=0)) + P(X=1)*(1 - P(X=1))  = 0.5

一个随机事件Y，P(Y=0) = 0.1 ,P(Y=1)=0.9

那么基尼不纯度就为P(Y=0)*(1 - P(Y=0)) +   P(Y=1)*(1 - P(Y=1))  = 0.18

很明显 X比Y更混乱，因为两个都为0.5 很难判断哪个发生。而Y就确定得多，Y=0的基尼不纯度很大。

**所以基尼不纯度也可以作为 衡量系统混乱程度的标准**

##### 3).信息熵与基尼系数差别

比起基尼系数，信息熵对不纯度**更加敏感**，对不纯度的**惩罚最强**。但是**在实际使用中，信息熵和基尼系数的效果基**
**本相同**。信息熵的计算比基尼系数**缓慢**一些，因为基尼系数的计算不涉及对数。另外，因为信息熵对不纯度更加敏
感，所以信息熵作为指标时，决策树的生长会更加“精细”，因此**对于高维数据或者噪音很多的数据，信息熵很容易**
**过拟合**，基尼系数在这种情况下效果往往比较好。当**模型拟合程度不足的时候，即当模型在训练集和测试集上都表**
**现不太好的时候，使用信息熵**。**维度低，数据比较清晰的时候，信息熵和基尼系数没区别**。当然，这些不是绝对的。

##### 建立一棵树代码

```python
#1. 导入需要的模块
from sklearn import tree
from sklearn.datasets import load_wine
from sklearn.model_selection import train_test_split

#2. 探索数据
wine = load_wine()

wine.data #数据

wine.target #标签

wine.data.shape #数据结构

import pandas as pd
pd.concat([pd.DataFrame(wine.data),pd.DataFrame(wine.target)],axis=1) #将wine拼接为表

wine.feature_names #特征名字

wine.target_names #标签名字

#3. 分训练集和测试集
Xtrain, Xtest, Ytrain, Ytest = train_test_split(wine.data,wine.target,test_size=0.3)
Xtrain.shape #训练集
Xtest.shape #测试集

#4. 建立模型
clf = tree.DecisionTreeClassifier(criterion="entropy") #选择信息熵
clf = clf.fit(Xtrain, Ytrain)
score = clf.score(Xtest, Ytest) #返回预测的准确度

score

#5. 画出一棵树吧
feature_name = ['酒精','苹果酸','灰','灰的碱性','镁','总酚','类黄酮','非黄烷类酚类','花青素','颜色强度','色调','od280/od315稀释葡萄酒','脯氨酸']
dot_data = tree.export_graphviz(clf
                               ,out_file = None
                               ,feature_names= feature_name
                               ,class_names=["01","02","03"]
                               ,filled=True #颜色:True为有颜色,颜色越深，不纯度越低
                               ,rounded=True #框的形状:True为有圆角
                               )
graph = graphviz.Source(dot_data)

graph

#6. 探索决策树
clf.feature_importances_

[*zip(feature_name,clf.feature_importances_) #根节点贡献度是最高的
 
clf = tree.DecisionTreeClassifier(criterion="entropy",random_state=30) #随机数种子
clf = clf.fit(Xtrain, Ytrain)
score = clf.score(Xtest, Ytest) #返回预测的准确度
 
score
```

#### 2.1.2 random_state & splitter

random_state用来设置分枝中的随机模式的参数，默认None，在**高维度时随机性会表现更明显**，**低维度的数据**
**（比如鸢尾花数据集），随机性几乎不会显现**。输入任意整数，会一直长出同一棵树，让模型稳定下来。

splitter也是用来控制决策树中的随机选项的，有两种输入值，**输入”best"，决策树在分枝时虽然随机，但是还是会优先选择更重要的特征进行分枝**（重要性可以通过属性feature_importances_查看），**输入“random"，决策树在分枝时会更加随机**，树会因为含有更多的不必要信息而更深更大，并因这些不必要信息而降低对训练集的拟合。这也是防止过拟合的一种方式。当你预测到你的模型会过拟合，用这两个参数来帮助你降低树建成之后过拟合的可能性。当然，树一旦建成，我们依然是使用剪枝参数来防止过拟合。

##### 固定结果代码

```python
clf = tree.DecisionTreeClassifier(criterion="entropy"
                                 ,random_state=30
                                 ,splitter="random"
                                 )
clf = clf.fit(Xtrain, Ytrain)
score = clf.score(Xtest, Ytest)

score

import graphviz
dot_data = tree.export_graphviz(clf
                               ,feature_names= feature_name
                               ,class_names=["01","02","03"]
                               ,filled=True
                               ,rounded=True
                               )  
graph = graphviz.Source(dot_data)

graph
```

#### 2.1.3剪枝参数

##### max_depth

**限制树的最大深度，超过设定深度的树枝全部剪掉**
这是用得最广泛的剪枝参数，在高维度低样本量时非常有效。决策树多生长一层，对样本量的需求会增加一倍，所
以限制树深度能够有效地限制过拟合。在集成算法中也非常实用。实际使用时，**建议从=3开始尝试**，看看拟合的效果再决定是否增加设定深度。

##### min_samples_leaf & min_samples_split

min_samples_leaf限定，一个节点在分枝后的每个子节点都必须包含至少min_samples_leaf个训练样本，否则分
枝就不会发生，或者，分枝会朝着满足每个子节点都包含min_samples_leaf个样本的方向去发生。
一般搭配max_depth使用，在回归树中有神奇的效果，可以让模型变得更加平滑。这个参数的**数量设置得太小会引起过拟合，设置得太大就会阻止模型学习数据**。一般来说，**建议从=5开始使用**。如果叶节点中含有的样本量变化很大，建议输入浮点数作为样本量的百分比来使用。同时，这个参数可以保证每个叶子的最小尺寸，可以在回归问题中避免低方差，过拟合的叶子节点出现。对于类别不多的分类问题，=1通常就是最佳选择。

min_samples_split限定，一个节点必须要包含至少min_samples_split个训练样本，这个节点才允许被分枝，否则分枝就不会发生。

##### 剪枝参数代码

```python
clf = tree.DecisionTreeClassifier(criterion="entropy"
                                 ,random_state=30
                                 ,splitter="random"
                                 ,max_depth=3 #树的最大深度
                                 ,min_samples_leaf=10 #节点分支后最小样本满足数
                                 ,min_samples_split=10 #节点分支后最小样本满足数
                                 )
clf = clf.fit(Xtrain, Ytrain)

dot_data = tree.export_graphviz(clf
                               ,feature_names= feature_name
                               ,class_names=["琴酒","雪莉","贝尔摩德"]
                               ,filled=True
                               ,rounded=True
                               )  
graph = graphviz.Source(dot_data)

graph

clf.score(Xtrain,Ytrain)
clf.score(Xtest,Ytest)
```

##### 确认最优的剪枝参数代码

```python
import matplotlib.pyplot as plt
test = []
for i in range(10):
    clf = tree.DecisionTreeClassifier(max_depth=i+1
                                     ,criterion="entropy"
                                     ,random_state=30
                                     ,splitter="random"
                                     )
    clf = clf.fit(Xtrain, Ytrain)
    score = clf.score(Xtest, Ytest)
    test.append(score)
plt.plot(range(1,11),test,color="red",label="max_depth")
plt.legend()
plt.show()
```

#### 注:

无论如何，剪枝参数的默认值会让树无尽地生长，这些树在某些数据集上可能非常巨大，对内存的消耗也非常巨
大。所以如果你手中的数据集非常巨大，你已经预测到无论如何你都是要剪枝的，那提前设定这些参数来控制树的
复杂性和大小会比较好。

#### 2.1.4 目标权重参数

##### class_weight & min_weight_fraction_leaf

完成样本标签平衡的参数。样本不平衡是指在一组数据集中，标签的一类天生占有很大的比例。比如说，在银行要
判断“一个办了信用卡的人是否会违约”，就是是vs否（1%：99%）的比例。这种分类状况下，即便模型什么也不
做，全把结果预测成“否”，正确率也能有99%。因此我们要使用class_weight参数对样本标签进行一定的均衡，给
少量的标签更多的权重，让模型更偏向少数类，向捕获少数类的方向建模。该参数默认None，此模式表示自动给
与数据集中的所有标签相同的权重。
有了权重之后，样本量就不再是单纯地记录数目，而是受输入的权重影响了，因此这时候剪枝，就需要搭配min_
weight_fraction_leaf这个基于权重的剪枝参数来使用。另请注意，基于权重的剪枝参数（例如min_weight_
fraction_leaf）将比不知道样本权重的标准（比如min_samples_leaf）更少偏向主导类。如果样本是加权的，则使用基于权重的预修剪标准来更容易优化树结构，这确保叶节点至少包含样本权重的总和的一小部分。

### 2.2 重要属性和接口

属性是在模型训练之后，能够调用查看的模型的各种性质。对决策树来说，最重要的是feature_importances_，能够查看各个特征对模型的重要性。

sklearn中许多算法的接口都是相似的，比如说我们之前已经用到的fit和score，几乎对每个算法都可以使用。除了
这两个接口之外，决策树最常用的接口还有apply和predict。apply中输入测试集返回每个测试样本所在的叶子节
点的索引，predict输入测试集返回每个测试样本的标签。

在这里不得不提的是，所有接口中要求输入X_train和X_test的部分，**输入的特征矩阵必须至少是一个<font color=#FF0000 >二维矩阵</font>。**
**sklearn不接受任何一维矩阵作为特征矩阵被输入。**如果你的数据的确只有一个特征，那必须用reshape(-1,1)来给
矩阵增维；如果你的数据只有一个特征和一个样本，使用reshape(1,-1)来给你的数据增维。

#### 代码

```python
#apply返回每个测试样本所在的叶子节点的索引
clf.apply(Xtest)
#predict返回每个测试样本的分类/回归结果
clf.predict(Xtest)
```

#### 注:

决策树天生不擅长环形数据

## 3 DecisionTreeRegressor(回归树)与波士顿房价数据集

### 3.1 重要参数，属性及接口

##### criterion

回归树衡量分枝质量的指标，支持的标准有三种：
**1.**输入<font color=#FF0000 >"mse"</font>使用**均方误差**mean squared error(MSE)，父节点和叶子节点之间的均方误差的差额将被用来作为
特征选择的标准，这种方法通过使用叶子节点的均值来最小化**L2损失**

**2.**输入<font color=#FF0000 >“friedman_mse”</font>使用费尔德曼均方误差，这种指标使用弗里德曼针对潜在分枝中的问题改进后的均方误差

**3.**输入<font color=#FF0000 >"mae"</font>使用绝对平均误差MAE（mean absolute error），这种指标使用叶节点的中值来最小化**L1损失**
属性中最重要的依然是feature_importances_，接口依然是apply, fit, predict, score最核心

#### 注:

**1.均方误差:**是预测值与真实值之差的平方和的平均值

![002](D:\Machine_Learning\sklearn\决策树\002.png)

**2.L1损失:**也被称为最小绝对值偏差（LAD），最小绝对值误差（LAE）。总的说来，它是把目标值（YiYi）与估计值（f(xi)f(xi)）的绝对差值的总和（SS）最小化

![003](D:\Machine_Learning\sklearn\决策树\003.png)

**3.L2损失:**也被称为最小平方误差（LSE）。总的来说，它是把目标值（YiYi）与估计值（f(xi)f(xi)）的差值的平方和（SS）最小化

![004](D:\Machine_Learning\sklearn\决策树\004.png)

**4.MSE的本质:**是样本真实数据与回归结果的差异。在回归树中，MSE不只是我们的分枝质量衡量指标，也是我们最常用的衡量回归树回归质量的指标。

**5.R平方:**当我们在使用交叉验证，或者其他方式获取回归树的结果时，我们往往选择均方误差作为我们的评估（在分类树中这个指标是score代表的预测准确率）。在回归中，我们追求的是，MSE越小越好。
然而，**回归树的接口score返回的是R平方，并不是MSE**。R平方被定义如下:

![005](D:\Machine_Learning\sklearn\决策树\005.png)

其中u是残差平方和（MSE * N），v是总平方和，N是样本数量，i是每一个数据样本，fi是模型回归出的数值，yi
是样本点i实际的数值标签。y帽是真实数值标签的平均数。**R平方可以为正为负**（如果模型的残差平方和远远大于
模型的总平方和，模型非常糟糕，R平方就会为负），**而均方误差永远为正**。

值得一提的是，**虽然均方误差永远为正，但是sklearn当中使用均方误差作为评判标准时，却是计算”负均方误**
**差“（neg_mean_squared_error）**。这是因为sklearn在计算模型评估指标的时候，会考虑指标本身的性质，均
方误差本身是一种误差，所以被sklearn划分为模型的一种损失(loss)，因此**在sklearn当中，都以负数表示**。真正的均方误差MSE的数值，其实就是neg_mean_squared_error去掉负号的数字。

### 3.2 交叉验证代码

```python
from sklearn.datasets import load_boston
from sklearn.model_selection import cross_val_score
from sklearn.tree import DecisionTreeRegressor

#交叉验证cross_val_score的用法
boston = load_boston()
regressor = DecisionTreeRegressor(random_state=0)
cross_val_score(regressor #模型，这里为回归
                ,boston.data #完整数据集，回归为连续数据，不需要划分测试集与训练集
                ,boston.target #完整标签
                ,cv=10 #交叉验证，把数据分十份，将其中1份作为测试集，其余9份作为训练集，
                ,scoring = "neg_mean_squared_error" #对于回归，填上返回负均方误差，不填返回R平方
               )
```

### 3.3 一维回归的图像绘制代码

```python
#1. 导入需要的库
import numpy as np
from sklearn.tree import DecisionTreeRegressor
import matplotlib.pyplot as plt

#2. 创建一条含有噪声的正弦曲线
rng = np.random.RandomState(1) #生成随机数种子
X = np.sort(5 * rng.rand(80,1), axis=0) #生成0-5的横坐标，且为二维数组
y = np.sin(X).ravel() #将sin函数值求出来并降维，一维数组不分行列即80个对象
y[::5] += 3 * (0.5 - rng.rand(16)) #每隔5个数生成在-1.5到1.5之间的噪声数据，总计16个

#3. 实例化&训练模型
regr_1 = DecisionTreeRegressor(max_depth=2)
regr_2 = DecisionTreeRegressor(max_depth=5)
regr_1.fit(X, y)
regr_2.fit(X, y)

#4. 测试集导入模型，预测结果
X_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis] #np.arrange(开始点，结束点，步长) 生成有序数组的函数
y_1 = regr_1.predict(X_test)
y_2 = regr_2.predict(X_test)
#了解增维切片np.newaxis的用法
l = np.array([1,2,3,4])
l
l.shape
l[:,np.newaxis]
l[:,np.newaxis].shape
l[np.newaxis,:].shape

#5. 绘制图像
plt.figure()
plt.scatter(X, y, s=20, edgecolor="black",c="darkorange", label="data")
plt.plot(X_test, y_1, color="cornflowerblue",label="max_depth=2", linewidth=2)
plt.plot(X_test, y_2, color="yellowgreen", label="max_depth=5", linewidth=2)
plt.xlabel("data")
plt.ylabel("target")
plt.title("Decision Tree Regression")
plt.legend()
plt.show()
```

![006](D:\Machine_Learning\sklearn\决策树\006.png)

#### 注:

回归树学习了近似正弦曲线的局部线性回归。如果树的最大深度（由max_depth参数控制）设置得太高，则决策树学习得太精细，它从训练数据中学了很多细节，包括噪声得呈现，从而使模型偏离真实的正弦曲线，形成过拟合

## 4 实例：泰坦尼克号幸存者的预测

### 代码:

```python
#1. 导入所需要的库
import numpy as np
import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import cross_val_score
import matplotlib.pyplot as plt

#2. 导入数据集，探索数据
data = pd.read_csv("./data.csv",index_col= 0)
data.head()
data.info()

#3. 对数据集进行预处理
#删除缺失值过多的列，和观察判断来说和预测的y没有关系的列
data.drop(["Cabin","Name","Ticket"],inplace=True,axis=1)

#处理缺失值，对缺失值较多的列进行填补，有一些特征只确实一两个值，可以采取直接删除记录的方法
data["Age"] = data["Age"].fillna(data["Age"].mean())
data = data.dropna()

#将分类变量转换为数值型变量

#将二分类变量转换为数值型变量
data["Sex"] = (data["Sex"]== "male").astype("int")

#将三分类变量转换为数值型变量
labels = data["Embarked"].unique().tolist()
data["Embarked"] = data["Embarked"].apply(lambda x: labels.index(x))

#查看处理后的数据集
data.head()

#4. 提取标签和特征矩阵，分测试集和训练集
X = data.iloc[:,data.columns != "Survived"]
y = data.iloc[:,data.columns == "Survived"]
from sklearn.model_selection import train_test_split
Xtrain, Xtest, Ytrain, Ytest = train_test_split(X,y,test_size=0.3)
#修正测试集和训练集的索引
for i in [Xtrain, Xtest, Ytrain, Ytest]:
    i.index = range(i.shape[0])
    
#查看分好的训练集和测试集
Xtrain.head()

#5. 导入模型，粗略跑一下查看结果
clf = DecisionTreeClassifier(random_state=25)
clf = clf.fit(Xtrain, Ytrain)
score_ = clf.score(Xtest, Ytest)
score_
score = cross_val_score(clf,X,y,cv=10).mean()
score

#6. 在不同max_depth下观察模型的拟合状况
tr = []
te = []
for i in range(10):
    clf = DecisionTreeClassifier(random_state=25
                                 ,max_depth=i+1
                                 ,criterion="entropy"
                               )
    clf = clf.fit(Xtrain, Ytrain)
    score_tr = clf.score(Xtrain,Ytrain)
    score_te = cross_val_score(clf,X,y,cv=10).mean()
    tr.append(score_tr)
    te.append(score_te)
print(max(te))
plt.plot(range(1,11),tr,color="red",label="train")
plt.plot(range(1,11),te,color="blue",label="test")
plt.xticks(range(1,11))
plt.legend()
plt.show()
#这里为什么使用“entropy”？因为我们注意到，在最大深度=3的时候，模型拟合不足，在训练集和测试集上的表现接近，但却都不是非常理想，只能够达到83%左右，所以我们要使用entropy。

#7. 用网格搜索调整参数
gini_thresholds = np.linspace(0,0.5,20)
parameters = {'splitter':('best','random')
             ,'criterion':("gini","entropy")
             ,"max_depth":[*range(1,10)]
             ,'min_samples_leaf':[*range(1,50,5)]
             ,'min_impurity_decrease':[*np.linspace(0,0.5,20)]
             }
clf = DecisionTreeClassifier(random_state=25)
GS = GridSearchCV(clf, parameters, cv=10)
GS.fit(Xtrain,Ytrain)
GS.best_params_
GS.best_score_
```

#### 注:

网格搜索所有参数都会使用，可能结果比使用部分参数还要差

## 5  决策树的优缺点

### 决策树优点

1. 易于理解和解释，因为树木可以画出来被看见
2. 需要很少的数据准备。其他很多算法通常都需要数据规范化，需要创建虚拟变量并删除空值等。但请注意，
sklearn中的决策树模块不支持对缺失值的处理。
3. 使用树的成本（比如说，在预测数据的时候）是用于训练树的数据点的数量的对数，相比于其他算法，这是
一个很低的成本。
4. 能够同时处理数字和分类数据，既可以做回归又可以做分类。其他技术通常专门用于分析仅具有一种变量类
型的数据集。
5. 能够处理多输出问题，即含有多个标签的问题，注意与一个标签中含有多种标签分类的问题区别开
6. 是一个白盒模型，结果很容易能够被解释。如果在模型中可以观察到给定的情况，则可以通过布尔逻辑轻松
解释条件。相反，在黑盒模型中（例如，在人工神经网络中），结果可能更难以解释。
7. 可以使用统计测试验证模型，这让我们可以考虑模型的可靠性。
8. 即使其假设在某种程度上违反了生成数据的真实模型，也能够表现良好。
决策树的缺点
1. 决策树学习者可能创建过于复杂的树，这些树不能很好地推广数据。这称为过度拟合。修剪，设置叶节点所
需的最小样本数或设置树的最大深度等机制是避免此问题所必需的，而这些参数的整合和调整对初学者来说
会比较晦涩
2. 决策树可能不稳定，数据中微小的变化可能导致生成完全不同的树，这个问题需要通过集成算法来解决。
3. 决策树的学习是基于贪婪算法，它靠优化局部最优（每个节点的最优）来试图达到整体的最优，但这种做法
不能保证返回全局最优决策树。这个问题也可以由集成算法来解决，在随机森林中，特征和样本会在分枝过
程中被随机采样。
4. 有些概念很难学习，因为决策树不容易表达它们，例如XOR，奇偶校验或多路复用器问题。
5. 如果标签中的某些类占主导地位，决策树学习者会创建偏向主导类的树。因此，建议在拟合决策树之前平衡
数据集。

### 决策树的缺点

1. 决策树学习者可能创建过于复杂的树，这些树不能很好地推广数据。这称为过度拟合。修剪，设置叶节点所
需的最小样本数或设置树的最大深度等机制是避免此问题所必需的，而这些参数的整合和调整对初学者来说
会比较晦涩
2. 决策树可能不稳定，数据中微小的变化可能导致生成完全不同的树，这个问题需要通过集成算法来解决。
3. 决策树的学习是基于贪婪算法，它靠优化局部最优（每个节点的最优）来试图达到整体的最优，但这种做法
不能保证返回全局最优决策树。这个问题也可以由集成算法来解决，在随机森林中，特征和样本会在分枝过
程中被随机采样。
4. 有些概念很难学习，因为决策树不容易表达它们，例如XOR，奇偶校验或多路复用器问题。
5. 如果标签中的某些类占主导地位，决策树学习者会创建偏向主导类的树。因此，建议在拟合决策树之前平衡
数据集。